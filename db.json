{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/typo/source/icon.svg","path":"icon.svg","modified":0,"renderable":1},{"_id":"themes/typo/source/css/post.css","path":"css/post.css","modified":0,"renderable":1},{"_id":"themes/typo/source/css/root.css","path":"css/root.css","modified":0,"renderable":1},{"_id":"themes/typo/source/css/style.css","path":"css/style.css","modified":0,"renderable":1},{"_id":"themes/typo/source/js/highlight.js","path":"js/highlight.js","modified":0,"renderable":1},{"_id":"themes/typo/source/js/theme.js","path":"js/theme.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/about.md","hash":"8d557ec671c23c85467488d555dc811896cd2cd6","modified":1752030314764},{"_id":"source/about/index.md","hash":"c80d1dbef1c27bfe59576ca2ff42d55dcc35ccf8","modified":1752030337027},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1752028487532},{"_id":"source/.DS_Store","hash":"53173cdb301dc2ef313d9e3997eca4abaacc448c","modified":1752290652436},{"_id":"source/about/me.md","hash":"7fb7fb00bf4f74ab26a6c156dc5bcf9d8cf88119","modified":1752030423202},{"_id":"source/about/me-1.md","hash":"793848432f378b5727bac18d03b35e8f2af5c82e","modified":1752030451253},{"_id":"themes/typo/.DS_Store","hash":"8898dc8300ee6e9ea503d197fcc23b790aa111c4","modified":1752034787144},{"_id":"themes/typo/README.md","hash":"d2f7c834240511ada4b3fed3d15b144aae950bb4","modified":1738669334000},{"_id":"themes/typo/package.json","hash":"5ea19d640f24551b373dade0a9c8314af752ce27","modified":1738669334000},{"_id":"themes/typo/layout/archive.ejs","hash":"471055a70e5a1f6de3d072936afc2f89142b840a","modified":1738669334000},{"_id":"themes/typo/layout/index.ejs","hash":"94d0ac16872060f1fa1f641cd419a4902b581347","modified":1738669334000},{"_id":"themes/typo/README.en.md","hash":"f35cb0f814b351a420212c47edadb2b08b91bbdb","modified":1738669334000},{"_id":"themes/typo/layout/layout.ejs","hash":"a9588db85067e7094503d1f015dc92c2b2499326","modified":1738669334000},{"_id":"themes/typo/source/.DS_Store","hash":"e9774d2337b17b1109d6fcc3282bb58c1b355c1f","modified":1752034787144},{"_id":"themes/typo/_config.yaml","hash":"399c793fde4b5e66225caf05b68c1a023e418a6e","modified":1738669334000},{"_id":"themes/typo/layout/post.ejs","hash":"de2b816d655477552ab94e70f49391bb9b4b121a","modified":1738669334000},{"_id":"themes/typo/layout/_partial/paginator.ejs","hash":"7ce58887f6601a8761333c579777ce35c8fe671f","modified":1738669334000},{"_id":"themes/typo/layout/_partial/header.ejs","hash":"64ca73942d3ccfb51f15e96b81d143640c61202c","modified":1738669334000},{"_id":"themes/typo/layout/_partial/footer.ejs","hash":"1f935caca6f8075486519ea93cc2270d695d24c7","modified":1738669334000},{"_id":"themes/typo/source/icon.svg","hash":"ffd23dde9841b012dfa273cbb45b1228a4c99481","modified":1738669334000},{"_id":"themes/typo/layout/_partial/head.ejs","hash":"ad8906eddcf324723bd0f881d8b3de443fc84af0","modified":1738669334000},{"_id":"themes/typo/source/css/post.css","hash":"9bd3fef37862088c87f1295081a0dcaba308cdb3","modified":1738669334000},{"_id":"themes/typo/source/css/root.css","hash":"c85efa24678e169e17f04570c493b3788f4ad424","modified":1738669334000},{"_id":"themes/typo/source/js/theme.js","hash":"2b84143778c121c875d2f4851cb0bb65f2f9f0b9","modified":1738669334000},{"_id":"themes/typo/source/js/highlight.js","hash":"4e9cab6a1aef35cf597582d17f14d027bab51200","modified":1738669334000},{"_id":"themes/typo/source/css/style.css","hash":"e7890edb5f81d4581e6cd0e769a5b8c018d4a149","modified":1738669334000},{"_id":"source/_posts/torch的封装层次.md","hash":"478f0a350ad4f361e9585998711d4bbb65963e59","modified":1752308500240}],"Category":[{"name":"技术实践","_id":"cmczzhsoo00015o9ldd63c9sh"}],"Data":[],"Page":[],"Post":[{"title":"torch的封装层次","date":"2025-07-12T08:21:40.000Z","_content":"\n> Simon: 作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构[github](https://github.com/pytorch/pytorch)，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。  \n\n\n> 0. cuda算子，一般是由核函数组成的.cu和.cpp文件     \n> 1. cuda封装，提供参数，调用算子，不进行存储    \n> 2. autograd算子，存储求导所需要的临时变量    \n> 3. function封装，在上一层的基础上做一些健壮性工作    \n> 4. Module封装，把函数封装成类，为的是实现永久性存储  \n\ntorch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？  \n-------------------------------------------------------------------\n\n1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫[cuda算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity)，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）\n\n这个封装层级下你可能会看到这样的调用方式：\n\n```text\nGEMM_cuda.fwd(mat1,mat2)\nGEMM_cuda.bwd(grad_o,mat1,mat2)\n```\n\n这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。\n\nc++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。\n\n但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y=wx，计算w的导数：dy/dw = x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。  \n\n---  \n\n2: 在cuda算子之上的2级封装是[autograd算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity)，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。\n\n你可能见到这种形式的autograd算子：\n\n```text\nimport torch\nclass TensorModelParallelCrossEntropy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, target, label_smoothing=0.0):\n        # do something\n        ctx.save_for_backward(...)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # do something\n        ... = ctx.saved_tensors\n        return grad_input, None, None\n```\n\n你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。\n\n这种算子可以通过下面这种方式调用：\n\n```text\nce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)\n```\n\n你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。  \n\n---  \n\n3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：\n\n```text\ndef linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):\n  assert input.is_cuda() and weight.is_cuda()\n  if lora_a is not None and lora_b is not None:\n    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)\n  else:\n    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)\n```\n\n[torch.nn](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity).functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：\n\n```text\nfrom torch.nn.functional import linear,dropout\nlinear(input,weight,bias)\ndropout(input,p=0.1,training=True)\n```\n\n但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。\n\n比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。\n\n---  \n\n4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：\n\n```text\nclass Linear(torch.nn.Module):\n  \n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n```\n\n它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：\n\n```text\nlayer = Linear(10,5,bias=False)\nx = torch.randn(2,10)\ny = layer(x)\n```\n\n[Module封装](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity)和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。\n\n到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。\n\n> 作者：真中合欢\n> 链接：https://www.zhihu.com/question/677187311/answer/3780895706\n","source":"_posts/torch的封装层次.md","raw":"---\ntitle: torch的封装层次\ncategories:\n  - 技术实践\ntag:\n  - 转载\n  - pytorch\ndate: 2025-07-12 16:21:40\ntags:\n---\n\n> Simon: 作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构[github](https://github.com/pytorch/pytorch)，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。  \n\n\n> 0. cuda算子，一般是由核函数组成的.cu和.cpp文件     \n> 1. cuda封装，提供参数，调用算子，不进行存储    \n> 2. autograd算子，存储求导所需要的临时变量    \n> 3. function封装，在上一层的基础上做一些健壮性工作    \n> 4. Module封装，把函数封装成类，为的是实现永久性存储  \n\ntorch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？  \n-------------------------------------------------------------------\n\n1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫[cuda算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity)，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）\n\n这个封装层级下你可能会看到这样的调用方式：\n\n```text\nGEMM_cuda.fwd(mat1,mat2)\nGEMM_cuda.bwd(grad_o,mat1,mat2)\n```\n\n这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。\n\nc++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。\n\n但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y=wx，计算w的导数：dy/dw = x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。  \n\n---  \n\n2: 在cuda算子之上的2级封装是[autograd算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity)，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。\n\n你可能见到这种形式的autograd算子：\n\n```text\nimport torch\nclass TensorModelParallelCrossEntropy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, target, label_smoothing=0.0):\n        # do something\n        ctx.save_for_backward(...)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # do something\n        ... = ctx.saved_tensors\n        return grad_input, None, None\n```\n\n你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。\n\n这种算子可以通过下面这种方式调用：\n\n```text\nce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)\n```\n\n你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。  \n\n---  \n\n3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：\n\n```text\ndef linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):\n  assert input.is_cuda() and weight.is_cuda()\n  if lora_a is not None and lora_b is not None:\n    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)\n  else:\n    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)\n```\n\n[torch.nn](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity).functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：\n\n```text\nfrom torch.nn.functional import linear,dropout\nlinear(input,weight,bias)\ndropout(input,p=0.1,training=True)\n```\n\n但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。\n\n比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。\n\n---  \n\n4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：\n\n```text\nclass Linear(torch.nn.Module):\n  \n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n```\n\n它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：\n\n```text\nlayer = Linear(10,5,bias=False)\nx = torch.randn(2,10)\ny = layer(x)\n```\n\n[Module封装](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity)和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。\n\n到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。\n\n> 作者：真中合欢\n> 链接：https://www.zhihu.com/question/677187311/answer/3780895706\n","slug":"torch的封装层次","published":1,"updated":"2025-07-12T08:21:40.240Z","comments":1,"layout":"post","photos":[],"_id":"cmczzhsok00005o9l2cnl076p","content":"<blockquote>\n<p>Simon: 作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构<a href=\"https://github.com/pytorch/pytorch\">github</a>，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。  </p>\n</blockquote>\n<blockquote>\n<ol start=\"0\">\n<li>cuda算子，一般是由核函数组成的.cu和.cpp文件     </li>\n<li>cuda封装，提供参数，调用算子，不进行存储    </li>\n<li>autograd算子，存储求导所需要的临时变量    </li>\n<li>function封装，在上一层的基础上做一些健壮性工作    </li>\n<li>Module封装，把函数封装成类，为的是实现永久性存储</li>\n</ol>\n</blockquote>\n<h2 id=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\"><a href=\"#torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\" class=\"headerlink\" title=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？  \"></a>torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？  </h2><p>1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity\">cuda算子</a>，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）</p>\n<p>这个封装层级下你可能会看到这样的调用方式：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GEMM_cuda.fwd(mat1,mat2)</span><br><span class=\"line\">GEMM_cuda.bwd(grad_o,mat1,mat2)</span><br></pre></td></tr></table></figure>\n\n<p>这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。</p>\n<p>c++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。</p>\n<p>但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y&#x3D;wx，计算w的导数：dy&#x2F;dw &#x3D; x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。  </p>\n<hr>\n<p>2: 在cuda算子之上的2级封装是<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity\">autograd算子</a>，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。</p>\n<p>你可能见到这种形式的autograd算子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">class TensorModelParallelCrossEntropy(torch.autograd.Function):</span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def forward(ctx, logits, target, label_smoothing=0.0):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ctx.save_for_backward(...)</span><br><span class=\"line\">        return loss</span><br><span class=\"line\"></span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def backward(ctx, grad_output):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ... = ctx.saved_tensors</span><br><span class=\"line\">        return grad_input, None, None</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。</p>\n<p>这种算子可以通过下面这种方式调用：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。  </p>\n<hr>\n<p>3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):</span><br><span class=\"line\">  assert input.is_cuda() and weight.is_cuda()</span><br><span class=\"line\">  if lora_a is not None and lora_b is not None:</span><br><span class=\"line\">    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)</span><br><span class=\"line\">  else:</span><br><span class=\"line\">    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity\">torch.nn</a>.functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from torch.nn.functional import linear,dropout</span><br><span class=\"line\">linear(input,weight,bias)</span><br><span class=\"line\">dropout(input,p=0.1,training=True)</span><br></pre></td></tr></table></figure>\n\n<p>但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。</p>\n<p>比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。</p>\n<hr>\n<p>4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Linear(torch.nn.Module):</span><br><span class=\"line\">  </span><br><span class=\"line\">    def __init__(self, in_features: int, out_features: int, bias: bool = True,</span><br><span class=\"line\">                 device=None, dtype=None) -&gt; None:</span><br><span class=\"line\">        factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.in_features = in_features</span><br><span class=\"line\">        self.out_features = out_features</span><br><span class=\"line\">        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))</span><br><span class=\"line\">        if bias:</span><br><span class=\"line\">            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            self.register_parameter(&#x27;bias&#x27;, None)</span><br><span class=\"line\">        self.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    def reset_parameters(self) -&gt; None:</span><br><span class=\"line\">        init.kaiming_uniform_(self.weight, a=math.sqrt(5))</span><br><span class=\"line\">        if self.bias is not None:</span><br><span class=\"line\">            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)</span><br><span class=\"line\">            bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0</span><br><span class=\"line\">            init.uniform_(self.bias, -bound, bound)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, input: Tensor) -&gt; Tensor:</span><br><span class=\"line\">        return F.linear(input, self.weight, self.bias)</span><br></pre></td></tr></table></figure>\n\n<p>它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Linear(10,5,bias=False)</span><br><span class=\"line\">x = torch.randn(2,10)</span><br><span class=\"line\">y = layer(x)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity\">Module封装</a>和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。</p>\n<p>到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。</p>\n<blockquote>\n<p>作者：真中合欢<br>链接：<a href=\"https://www.zhihu.com/question/677187311/answer/3780895706\">https://www.zhihu.com/question/677187311/answer/3780895706</a></p>\n</blockquote>\n","excerpt":"","more":"<blockquote>\n<p>Simon: 作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构<a href=\"https://github.com/pytorch/pytorch\">github</a>，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。  </p>\n</blockquote>\n<blockquote>\n<ol start=\"0\">\n<li>cuda算子，一般是由核函数组成的.cu和.cpp文件     </li>\n<li>cuda封装，提供参数，调用算子，不进行存储    </li>\n<li>autograd算子，存储求导所需要的临时变量    </li>\n<li>function封装，在上一层的基础上做一些健壮性工作    </li>\n<li>Module封装，把函数封装成类，为的是实现永久性存储</li>\n</ol>\n</blockquote>\n<h2 id=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\"><a href=\"#torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\" class=\"headerlink\" title=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？  \"></a>torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？  </h2><p>1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity\">cuda算子</a>，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）</p>\n<p>这个封装层级下你可能会看到这样的调用方式：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GEMM_cuda.fwd(mat1,mat2)</span><br><span class=\"line\">GEMM_cuda.bwd(grad_o,mat1,mat2)</span><br></pre></td></tr></table></figure>\n\n<p>这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。</p>\n<p>c++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。</p>\n<p>但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y&#x3D;wx，计算w的导数：dy&#x2F;dw &#x3D; x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。  </p>\n<hr>\n<p>2: 在cuda算子之上的2级封装是<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity\">autograd算子</a>，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。</p>\n<p>你可能见到这种形式的autograd算子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">class TensorModelParallelCrossEntropy(torch.autograd.Function):</span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def forward(ctx, logits, target, label_smoothing=0.0):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ctx.save_for_backward(...)</span><br><span class=\"line\">        return loss</span><br><span class=\"line\"></span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def backward(ctx, grad_output):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ... = ctx.saved_tensors</span><br><span class=\"line\">        return grad_input, None, None</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。</p>\n<p>这种算子可以通过下面这种方式调用：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。  </p>\n<hr>\n<p>3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):</span><br><span class=\"line\">  assert input.is_cuda() and weight.is_cuda()</span><br><span class=\"line\">  if lora_a is not None and lora_b is not None:</span><br><span class=\"line\">    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)</span><br><span class=\"line\">  else:</span><br><span class=\"line\">    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity\">torch.nn</a>.functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from torch.nn.functional import linear,dropout</span><br><span class=\"line\">linear(input,weight,bias)</span><br><span class=\"line\">dropout(input,p=0.1,training=True)</span><br></pre></td></tr></table></figure>\n\n<p>但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。</p>\n<p>比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。</p>\n<hr>\n<p>4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Linear(torch.nn.Module):</span><br><span class=\"line\">  </span><br><span class=\"line\">    def __init__(self, in_features: int, out_features: int, bias: bool = True,</span><br><span class=\"line\">                 device=None, dtype=None) -&gt; None:</span><br><span class=\"line\">        factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.in_features = in_features</span><br><span class=\"line\">        self.out_features = out_features</span><br><span class=\"line\">        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))</span><br><span class=\"line\">        if bias:</span><br><span class=\"line\">            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            self.register_parameter(&#x27;bias&#x27;, None)</span><br><span class=\"line\">        self.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    def reset_parameters(self) -&gt; None:</span><br><span class=\"line\">        init.kaiming_uniform_(self.weight, a=math.sqrt(5))</span><br><span class=\"line\">        if self.bias is not None:</span><br><span class=\"line\">            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)</span><br><span class=\"line\">            bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0</span><br><span class=\"line\">            init.uniform_(self.bias, -bound, bound)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, input: Tensor) -&gt; Tensor:</span><br><span class=\"line\">        return F.linear(input, self.weight, self.bias)</span><br></pre></td></tr></table></figure>\n\n<p>它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Linear(10,5,bias=False)</span><br><span class=\"line\">x = torch.randn(2,10)</span><br><span class=\"line\">y = layer(x)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity\">Module封装</a>和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。</p>\n<p>到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。</p>\n<blockquote>\n<p>作者：真中合欢<br>链接：<a href=\"https://www.zhihu.com/question/677187311/answer/3780895706\">https://www.zhihu.com/question/677187311/answer/3780895706</a></p>\n</blockquote>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cmczzhsok00005o9l2cnl076p","category_id":"cmczzhsoo00015o9ldd63c9sh","_id":"cmczzhsop00045o9l2jut50sl"}],"PostTag":[{"post_id":"cmczzhsok00005o9l2cnl076p","tag_id":"cmczzhsoo00025o9l86cier6r","_id":"cmczzhsop00055o9l6xvsb75t"},{"post_id":"cmczzhsok00005o9l2cnl076p","tag_id":"cmczzhsop00035o9l2vtm9if8","_id":"cmczzhsop00065o9lasbl5q7w"}],"Tag":[{"name":"转载","_id":"cmczzhsoo00025o9l86cier6r"},{"name":"pytorch","_id":"cmczzhsop00035o9l2vtm9if8"}]}}