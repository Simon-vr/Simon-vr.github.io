{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/typo/source/css/post.css","path":"css/post.css","modified":1,"renderable":1},{"_id":"themes/typo/source/css/root.css","path":"css/root.css","modified":1,"renderable":1},{"_id":"themes/typo/source/css/style.css","path":"css/style.css","modified":1,"renderable":1},{"_id":"themes/typo/source/js/highlight.js","path":"js/highlight.js","modified":1,"renderable":1},{"_id":"themes/typo/source/js/theme.js","path":"js/theme.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/_posts/TUDelt硕士申请调研.md","hash":"13c06c1a2a8655341c04fba28f211243c803d924","modified":1754623007592},{"_id":"source/_posts/TMU硕士申请调研.md","hash":"492c464d83787b8e09e9932d680e8bdc161210fb","modified":1757244895943},{"_id":"source/_posts/torch的封装层次.md","hash":"937b4bad01fe0df7313721ae9618309e52164a11","modified":1757242966047},{"_id":"source/.DS_Store","hash":"5fef2985a4eb172e694e6e2877d57d71fa35ab7e","modified":1754717049084},{"_id":"source/_posts/从《西征记略》看清代早期朝廷满汉语的使用.md","hash":"7fe076594a6975604a7f2697085c80ae0e681758","modified":1757243316746},{"_id":"source/_posts/二十年前的相机.md","hash":"e54381e5ab15cca1376091c31c23084cc192a624","modified":1757243716906},{"_id":"source/about/index.md","hash":"78f4573fe4ff02e7fbf0f91e333ae21fb284b93d","modified":1752322582311},{"_id":"source/_posts/.DS_Store","hash":"9b2c5c92061b80169fa9cfe0dca134f44ec126aa","modified":1754715056208},{"_id":"themes/typo/README.en.md","hash":"f35cb0f814b351a420212c47edadb2b08b91bbdb","modified":1738669334000},{"_id":"themes/typo/README.md","hash":"d2f7c834240511ada4b3fed3d15b144aae950bb4","modified":1738669334000},{"_id":"themes/typo/package.json","hash":"5ea19d640f24551b373dade0a9c8314af752ce27","modified":1738669334000},{"_id":"themes/typo/.DS_Store","hash":"c28dbe18f4fe8aadb90d942284705c5f30a0d367","modified":1752320893085},{"_id":"themes/typo/layout/index.ejs","hash":"94d0ac16872060f1fa1f641cd419a4902b581347","modified":1738669334000},{"_id":"themes/typo/layout/layout.ejs","hash":"a9588db85067e7094503d1f015dc92c2b2499326","modified":1738669334000},{"_id":"themes/typo/layout/archive.ejs","hash":"471055a70e5a1f6de3d072936afc2f89142b840a","modified":1738669334000},{"_id":"themes/typo/icon.svg","hash":"eb6911ca60b8c42a4433c523e751bb7226797a40","modified":1752321278230},{"_id":"themes/typo/layout/post.ejs","hash":"de2b816d655477552ab94e70f49391bb9b4b121a","modified":1738669334000},{"_id":"themes/typo/layout/_partial/footer.ejs","hash":"1f935caca6f8075486519ea93cc2270d695d24c7","modified":1738669334000},{"_id":"themes/typo/layout/_partial/head.ejs","hash":"ad8906eddcf324723bd0f881d8b3de443fc84af0","modified":1738669334000},{"_id":"themes/typo/layout/_partial/header.ejs","hash":"64ca73942d3ccfb51f15e96b81d143640c61202c","modified":1738669334000},{"_id":"themes/typo/layout/_partial/paginator.ejs","hash":"7ce58887f6601a8761333c579777ce35c8fe671f","modified":1738669334000},{"_id":"themes/typo/source/.DS_Store","hash":"e9774d2337b17b1109d6fcc3282bb58c1b355c1f","modified":1752034787144},{"_id":"themes/typo/_config.yaml","hash":"8393f2f88cc39f628ebb915041d2b5ec10f04cba","modified":1752321596028},{"_id":"themes/typo/source/js/highlight.js","hash":"4e9cab6a1aef35cf597582d17f14d027bab51200","modified":1738669334000},{"_id":"themes/typo/source/js/theme.js","hash":"2b84143778c121c875d2f4851cb0bb65f2f9f0b9","modified":1738669334000},{"_id":"themes/typo/source/css/style.css","hash":"e7890edb5f81d4581e6cd0e769a5b8c018d4a149","modified":1738669334000},{"_id":"themes/typo/source/css/root.css","hash":"c85efa24678e169e17f04570c493b3788f4ad424","modified":1738669334000},{"_id":"themes/typo/source/css/post.css","hash":"9bd3fef37862088c87f1295081a0dcaba308cdb3","modified":1738669334000},{"_id":"source/_posts/二十年前的相机/相机.jpg","hash":"df1ea692d6c7c58db02fdc17ae3f92b3388aac94","modified":1752756558542},{"_id":"source/_posts/TMU硕士申请调研/课程.png","hash":"21d6a6b5bf2f864c862f8b5dcea22cd3f9dae6ee","modified":1757243114416},{"_id":"source/_posts/二十年前的相机/内部.jpg","hash":"90712684a5c2505abb86d544b84ca2de28a827f0","modified":1752756558533},{"_id":"source/_posts/二十年前的相机/包装.jpg","hash":"60c133add5314efefbc9c93b2a90489631446266","modified":1752756558534},{"_id":"source/_posts/二十年前的相机/照片.jpg","hash":"67efc6ac23faa00d8c036ec2fe4c9a29e6d3e723","modified":1752756558536},{"_id":"public/about/index.html","hash":"17c4f7d0021fba9ec152a34a1c32cd9ff7b5e98d","modified":1757245984714},{"_id":"public/2025/08/07/TUDelt硕士申请调研/index.html","hash":"bcacc1fe00a901739c0cfa383c7d3b7223824fc0","modified":1757245984714},{"_id":"public/2025/07/17/TMU硕士申请调研/index.html","hash":"15ed64fa03ba7e045d795a5f2ba0612b48632afb","modified":1757245984714},{"_id":"public/2025/07/13/二十年前的相机/index.html","hash":"8bb79fc66ca325432c7ae15a5f3e7a03e3d08546","modified":1757245984714},{"_id":"public/2025/07/12/torch的封装层次/index.html","hash":"ed9929b900527a3827e83e0bd28fb598924126a5","modified":1757245984714},{"_id":"public/archives/index.html","hash":"aa701c7e2d99826cef6bece4a155d6c8e4776712","modified":1757245984714},{"_id":"public/archives/2025/index.html","hash":"aa701c7e2d99826cef6bece4a155d6c8e4776712","modified":1757245984714},{"_id":"public/archives/2025/07/index.html","hash":"4c6a95d4137c74f2377360e3d945cc04029186f4","modified":1757245984714},{"_id":"public/archives/2025/08/index.html","hash":"5c233550bbd44a3a195bb2e63e8c4d6904f69187","modified":1757245984714},{"_id":"public/categories/随笔杂记/index.html","hash":"a5b6b786c0a84aa313e86d262ff55b99ec0275e0","modified":1757245984714},{"_id":"public/categories/技术实践/index.html","hash":"122745ab726a152b21ec538da28d55c23365e085","modified":1757245984714},{"_id":"public/index.html","hash":"0846dabf10392b86cab3544dc1ac4838be79a1f1","modified":1757245984714},{"_id":"public/tags/留学申请/index.html","hash":"732b5ae715b9b1c528bd8867d202234014518163","modified":1757245984714},{"_id":"public/tags/转载/index.html","hash":"122745ab726a152b21ec538da28d55c23365e085","modified":1757245984714},{"_id":"public/tags/pytorch/index.html","hash":"122745ab726a152b21ec538da28d55c23365e085","modified":1757245984714},{"_id":"public/tags/摄影/index.html","hash":"b49fc0dcc00e84aadbe4e31ad55f6d9d3cf4ef24","modified":1757245984714},{"_id":"public/css/post.css","hash":"9bd3fef37862088c87f1295081a0dcaba308cdb3","modified":1757245984714},{"_id":"public/css/root.css","hash":"c85efa24678e169e17f04570c493b3788f4ad424","modified":1757245984714},{"_id":"public/css/style.css","hash":"e7890edb5f81d4581e6cd0e769a5b8c018d4a149","modified":1757245984714},{"_id":"public/js/highlight.js","hash":"4e9cab6a1aef35cf597582d17f14d027bab51200","modified":1757245984714},{"_id":"public/js/theme.js","hash":"2b84143778c121c875d2f4851cb0bb65f2f9f0b9","modified":1757245984714},{"_id":"public/2025/07/13/二十年前的相机/相机.jpg","hash":"df1ea692d6c7c58db02fdc17ae3f92b3388aac94","modified":1757245984714},{"_id":"public/2025/07/17/TMU硕士申请调研/课程.png","hash":"21d6a6b5bf2f864c862f8b5dcea22cd3f9dae6ee","modified":1757245984714},{"_id":"public/assets/js/DPlayer.min.js","hash":"290283e41ac69bfd570c90800680097f998e4e0c","modified":1757245984714},{"_id":"public/2025/07/13/二十年前的相机/内部.jpg","hash":"90712684a5c2505abb86d544b84ca2de28a827f0","modified":1757245984714},{"_id":"public/2025/07/13/二十年前的相机/包装.jpg","hash":"60c133add5314efefbc9c93b2a90489631446266","modified":1757245984714},{"_id":"public/2025/07/13/二十年前的相机/照片.jpg","hash":"67efc6ac23faa00d8c036ec2fe4c9a29e6d3e723","modified":1757245984714}],"Category":[{"name":"随笔杂记","_id":"cmf9mvzsu0003ad9l3cmyhcus"},{"name":"技术实践","_id":"cmf9mvzsx000bad9l3vd82zc0"}],"Data":[],"Page":[{"title":"about","date":"2025-07-12T12:00:32.000Z","_content":"> 造一个草原  \n> 需要一株苜蓿和一只蜜蜂  \n> 一株苜蓿、一只蜜蜂——\n> 再加一个梦\n> 要是蜜蜂少\n> 光有梦  \n> 也成  \n","source":"about/index.md","raw":"---\ntitle: about\ndate: 2025-07-12 20:00:32\n---\n> 造一个草原  \n> 需要一株苜蓿和一只蜜蜂  \n> 一株苜蓿、一只蜜蜂——\n> 再加一个梦\n> 要是蜜蜂少\n> 光有梦  \n> 也成  \n","updated":"2025-07-12T12:16:22.311Z","path":"about/index.html","comments":1,"layout":"page","_id":"cmf9mvzsr0000ad9l86ky5lwc","content":"<blockquote>\n<p>造一个草原<br>需要一株苜蓿和一只蜜蜂<br>一株苜蓿、一只蜜蜂——<br>再加一个梦<br>要是蜜蜂少<br>光有梦<br>也成  </p>\n</blockquote>\n","excerpt":"","more":"<blockquote>\n<p>造一个草原<br>需要一株苜蓿和一只蜜蜂<br>一株苜蓿、一只蜜蜂——<br>再加一个梦<br>要是蜜蜂少<br>光有梦<br>也成  </p>\n</blockquote>\n"}],"Post":[{"title":"TMU硕士申请调研","date":"2025-07-17T12:37:37.000Z","_content":"### 资料来源\n\n本文所有资料均来自于TMU官网中[School of Computation,Information and Technology板块](https://www.cit.tum.de/en/cit/home/)，以下简称CIT学院。\n其中具体信息主要来自于其中的[学位项目板块](https://www.cit.tum.de/en/cit/studies/degree-programs/)\n\n### 硕士学制\n\n标准学制2年，以课程为主\n\n### 在当地工作\n\n毕业后到慕尼黑外国人事务局（KVR）把原来的学生居留直接换成“18 个月求职居留”（§ 16b Abs. 4 AufenthG）。\n在这 18 个月内可以无限制打工（不限天数、不限行业），以维持生活并积累相关经验\n找到与学位匹配的“合格工作”后，可立即把求职居留转换成“就业居留”（§ 18b AufenthG）。\n只要这份工作持续 24 个月、并缴纳社保，且德语达到 A1，就能申请德国永久居留。（https://www.community.tum.de/wp-content/uploads/2023/04/TU_Flyer_WorkingInGermany2023_WEB.pdf）\n\n总的而言，德国还是有一定IT缺口的。\n\n### CIT本科专业\n\n信息来自于[学位项目板块](https://www.cit.tum.de/en/cit/studies/degree-programs/)\n\n- 生物信息学\n- 电气和计算机工程\n- 电子和数据工程（在新加坡TUM亚洲）\n- 信息学\n- 信息学：游戏工程\n- 信息工程\n- 信息系统\n- 数学\n\n### 本科信息学（Informatics）培养方案（用于检查课程匹配度）\n\n[网址](https://www.cit.tum.de/cit/studium/studiengaenge/bachelor-informatik/studienplan/)\n翻译如下：\n{% asset_img \"课程.png\"%}\n对个人而言，这个专业是最契合国内本科计算机科学专业培养方案的。\n\n### 本科信息工程（Information Engineering）培养方案\n\n[官网文件](https://www.cit.tum.de/fileadmin/w00byx/cit/Studium/Studiengaenge/Bachelor_Information_Engineering_Heilbronn/20221213_Studiengangsdokumentation_BSc_IE_Teil_A_AbgabeSenat__1_.pdf)\n主要内容翻译如下：\n\n| 类别                                                   | 学分  | 说明                                           |\n| ------------------------------------------------------ | ----- | ---------------------------------------------- |\n| **必修：计算机科学**                             | 90 CP | 信息学核心课程                                 |\n| **必修：数学**                                   | 36 CP | 离散结构、线性代数、微积分、概率论             |\n| **选修：计算机科学**                             | 12 CP | 从实时系统、虚拟机、密码学等模块中选 2 门      |\n| **选修：经济/管理**                              | 18 CP | 财务会计、管理科学、物流与生产等 3 门          |\n| **选修：跨学科/软技能**                          | 9 CP  | 伦理、创业、跨文化沟通等                       |\n| **毕业论文 + 答辩**                              | 15 CP | 第 6 学期完成（12 CP 论文 + 3 CP 答辩）        |\n| **必修实践项目**                                 | 10 CP | 第 5 学期小组项目（Bachelor Practical Course） |\n| **必修研讨课**                                   | 5 CP  | 第 3 学期独立完成科研小课题                    |\n| 添加了一些经管类课程，其他本科专业大同小异，不再赘述。 |       |                                                |\n\n### CIT硕士专业\n\n信息来自于[学位项目板块](https://www.cit.tum.de/en/cit/studies/degree-programs/)\n生物信息学\n生物医学计算\n通信和电子工程\n计算科学与工程\n数据工程和分析\n电气和计算机工程\n财务和信息管理（与TUM管理学院合作）\n绿色电子（在新加坡TUM亚洲）\n信息学\n信息学：游戏工程\n信息工程\n信息系统\n集成电路设计（在新加坡TUM亚洲）\n数学\n数学金融和精算科学\n数据科学中的数学\n运筹学中的数学\n科学与工程中的数学\n微电子和芯片设计\n神经工程——精英硕士课程\n机器人、认知、智能\n软件工程——精英研究生课程\nTopMath – 精英项目\n\n### 计算科学与工程（Computational Science and Engineering）\n\n这个项目主要以数学仿真为主。\n**Please note that CSE is not a computer science program, and students who wish to pursue such a program are not encouraged to apply to CSE. If you are interested in computer science or computer engineering, the Informatics Master's program is for you. The same is true if your primary interest is Robotics or Machine Learning, as these other programs would likely be a better fit for you.**\n对此不感兴趣，故掠过，感兴趣可[访问](https://www.cit.tum.de/en/cit/studies/degree-programs/master-computational-science-engineering/)\n\n### 机器人Robotics, Cognition, Intelligence\n\n[主页](https://www.cit.tum.de/en/cit/studium/studiengaenge/master-robotics-cognition-intelligence/#c2284)\n\n- [学费](https://www.tum.de/studium/studienfinanzierung/studiengebuehren-fuer-studierende-aus-nicht-eu-laendern#c124247)\n  6000欧元一学期，一年两学期，大多数工科都一样。\n- 标准学制：四个学期。\n- 教学语言：德语和英语（这就意味着需要德语成绩）\n  不符合语言条件，略过。\n\n### 信息学(Informatics)\n\n- [主页](https://www.cit.tum.de/en/cit/studies/degree-programs/master-informatics/)\n- 语言：英语\n- 申请期限\n  - 冬季学期：2月1日至5月31日\n  - 夏季学期：9月1日至11月30日\n- 申请流程[参考](https://www.tum.de/en/studies/application/master/application-master)\n  - [注册](https://campus.tum.de/tumonline/)\n  - 能力评估\n    - 在初始阶段，您在学士学位课程中获得的成绩以及您的书面文件将使用积分系统进行评估。根据累积的积分数量，申请人要么立即被录取，要么被拒绝，要么被邀请参加该部门进行的20分钟的招生面试。在某些情况下，对国际学生进行电话面试。\n    - 面试代表了程序的第二阶段，有助于确定申请人是否能够成功完成所需的学习课程。您可以在您期望的学位课程的学术和考试条例的附录2中找到更多信息。\n  - GRE分数要求：**GRE and GATE\n    Applicants with a Degree from Bangladesh, China, India, Iran or Pakistan have to submit a GRE (General) Test. We have defined required minimum scores, lower scores will not be accepted!\n    The required scores are:\n    Verbal reasoning: (will not be taken into account anymore)\n    Quantitative reasoning: 164\n    Analytical writing: 4.0**\n  - VPD：\n    德语全称：Vorprüfungsdokumentation\n    中文名称：德国大学入学资格预审核证明\n    由 Uni-Assist（德国高校国际合作申请服务中心）出具，用于提前审核非欧盟申请者的学历、成绩和学分是否符合德国大学的入学标准，并把成绩换算成德国计分体系。提前3–6个月向 uni-assist 提交材料，因为：uni-assist处理时间通常为 4–6周，高峰期可能更长；VPD有效期为 1年，但部分大学要求VPD必须在申请截止前不超过6个月内开具\n  - APS：\n    全称 德国驻华使馆文化处留德人员审核部（Akademische Prüfstelle，简称 APS）审核中国学生的学历、成绩单、学位证等材料是否真实有效。\n  - 命题小论文：科学论文/论文应长约1000字，并且必须用英语书写\n\n### 总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\n","source":"_posts/TMU硕士申请调研.md","raw":"---\ntitle: TMU硕士申请调研\ndate: 2025-07-17 20:37:37\ncategories: 随笔杂记\ntags: 留学申请\n---\n### 资料来源\n\n本文所有资料均来自于TMU官网中[School of Computation,Information and Technology板块](https://www.cit.tum.de/en/cit/home/)，以下简称CIT学院。\n其中具体信息主要来自于其中的[学位项目板块](https://www.cit.tum.de/en/cit/studies/degree-programs/)\n\n### 硕士学制\n\n标准学制2年，以课程为主\n\n### 在当地工作\n\n毕业后到慕尼黑外国人事务局（KVR）把原来的学生居留直接换成“18 个月求职居留”（§ 16b Abs. 4 AufenthG）。\n在这 18 个月内可以无限制打工（不限天数、不限行业），以维持生活并积累相关经验\n找到与学位匹配的“合格工作”后，可立即把求职居留转换成“就业居留”（§ 18b AufenthG）。\n只要这份工作持续 24 个月、并缴纳社保，且德语达到 A1，就能申请德国永久居留。（https://www.community.tum.de/wp-content/uploads/2023/04/TU_Flyer_WorkingInGermany2023_WEB.pdf）\n\n总的而言，德国还是有一定IT缺口的。\n\n### CIT本科专业\n\n信息来自于[学位项目板块](https://www.cit.tum.de/en/cit/studies/degree-programs/)\n\n- 生物信息学\n- 电气和计算机工程\n- 电子和数据工程（在新加坡TUM亚洲）\n- 信息学\n- 信息学：游戏工程\n- 信息工程\n- 信息系统\n- 数学\n\n### 本科信息学（Informatics）培养方案（用于检查课程匹配度）\n\n[网址](https://www.cit.tum.de/cit/studium/studiengaenge/bachelor-informatik/studienplan/)\n翻译如下：\n{% asset_img \"课程.png\"%}\n对个人而言，这个专业是最契合国内本科计算机科学专业培养方案的。\n\n### 本科信息工程（Information Engineering）培养方案\n\n[官网文件](https://www.cit.tum.de/fileadmin/w00byx/cit/Studium/Studiengaenge/Bachelor_Information_Engineering_Heilbronn/20221213_Studiengangsdokumentation_BSc_IE_Teil_A_AbgabeSenat__1_.pdf)\n主要内容翻译如下：\n\n| 类别                                                   | 学分  | 说明                                           |\n| ------------------------------------------------------ | ----- | ---------------------------------------------- |\n| **必修：计算机科学**                             | 90 CP | 信息学核心课程                                 |\n| **必修：数学**                                   | 36 CP | 离散结构、线性代数、微积分、概率论             |\n| **选修：计算机科学**                             | 12 CP | 从实时系统、虚拟机、密码学等模块中选 2 门      |\n| **选修：经济/管理**                              | 18 CP | 财务会计、管理科学、物流与生产等 3 门          |\n| **选修：跨学科/软技能**                          | 9 CP  | 伦理、创业、跨文化沟通等                       |\n| **毕业论文 + 答辩**                              | 15 CP | 第 6 学期完成（12 CP 论文 + 3 CP 答辩）        |\n| **必修实践项目**                                 | 10 CP | 第 5 学期小组项目（Bachelor Practical Course） |\n| **必修研讨课**                                   | 5 CP  | 第 3 学期独立完成科研小课题                    |\n| 添加了一些经管类课程，其他本科专业大同小异，不再赘述。 |       |                                                |\n\n### CIT硕士专业\n\n信息来自于[学位项目板块](https://www.cit.tum.de/en/cit/studies/degree-programs/)\n生物信息学\n生物医学计算\n通信和电子工程\n计算科学与工程\n数据工程和分析\n电气和计算机工程\n财务和信息管理（与TUM管理学院合作）\n绿色电子（在新加坡TUM亚洲）\n信息学\n信息学：游戏工程\n信息工程\n信息系统\n集成电路设计（在新加坡TUM亚洲）\n数学\n数学金融和精算科学\n数据科学中的数学\n运筹学中的数学\n科学与工程中的数学\n微电子和芯片设计\n神经工程——精英硕士课程\n机器人、认知、智能\n软件工程——精英研究生课程\nTopMath – 精英项目\n\n### 计算科学与工程（Computational Science and Engineering）\n\n这个项目主要以数学仿真为主。\n**Please note that CSE is not a computer science program, and students who wish to pursue such a program are not encouraged to apply to CSE. If you are interested in computer science or computer engineering, the Informatics Master's program is for you. The same is true if your primary interest is Robotics or Machine Learning, as these other programs would likely be a better fit for you.**\n对此不感兴趣，故掠过，感兴趣可[访问](https://www.cit.tum.de/en/cit/studies/degree-programs/master-computational-science-engineering/)\n\n### 机器人Robotics, Cognition, Intelligence\n\n[主页](https://www.cit.tum.de/en/cit/studium/studiengaenge/master-robotics-cognition-intelligence/#c2284)\n\n- [学费](https://www.tum.de/studium/studienfinanzierung/studiengebuehren-fuer-studierende-aus-nicht-eu-laendern#c124247)\n  6000欧元一学期，一年两学期，大多数工科都一样。\n- 标准学制：四个学期。\n- 教学语言：德语和英语（这就意味着需要德语成绩）\n  不符合语言条件，略过。\n\n### 信息学(Informatics)\n\n- [主页](https://www.cit.tum.de/en/cit/studies/degree-programs/master-informatics/)\n- 语言：英语\n- 申请期限\n  - 冬季学期：2月1日至5月31日\n  - 夏季学期：9月1日至11月30日\n- 申请流程[参考](https://www.tum.de/en/studies/application/master/application-master)\n  - [注册](https://campus.tum.de/tumonline/)\n  - 能力评估\n    - 在初始阶段，您在学士学位课程中获得的成绩以及您的书面文件将使用积分系统进行评估。根据累积的积分数量，申请人要么立即被录取，要么被拒绝，要么被邀请参加该部门进行的20分钟的招生面试。在某些情况下，对国际学生进行电话面试。\n    - 面试代表了程序的第二阶段，有助于确定申请人是否能够成功完成所需的学习课程。您可以在您期望的学位课程的学术和考试条例的附录2中找到更多信息。\n  - GRE分数要求：**GRE and GATE\n    Applicants with a Degree from Bangladesh, China, India, Iran or Pakistan have to submit a GRE (General) Test. We have defined required minimum scores, lower scores will not be accepted!\n    The required scores are:\n    Verbal reasoning: (will not be taken into account anymore)\n    Quantitative reasoning: 164\n    Analytical writing: 4.0**\n  - VPD：\n    德语全称：Vorprüfungsdokumentation\n    中文名称：德国大学入学资格预审核证明\n    由 Uni-Assist（德国高校国际合作申请服务中心）出具，用于提前审核非欧盟申请者的学历、成绩和学分是否符合德国大学的入学标准，并把成绩换算成德国计分体系。提前3–6个月向 uni-assist 提交材料，因为：uni-assist处理时间通常为 4–6周，高峰期可能更长；VPD有效期为 1年，但部分大学要求VPD必须在申请截止前不超过6个月内开具\n  - APS：\n    全称 德国驻华使馆文化处留德人员审核部（Akademische Prüfstelle，简称 APS）审核中国学生的学历、成绩单、学位证等材料是否真实有效。\n  - 命题小论文：科学论文/论文应长约1000字，并且必须用英语书写\n\n### 总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\n","slug":"TMU硕士申请调研","published":1,"updated":"2025-09-07T11:34:55.943Z","comments":1,"layout":"post","photos":[],"_id":"cmf9mvzss0001ad9l6adhdu5p","content":"<h3 id=\"资料来源\"><a href=\"#资料来源\" class=\"headerlink\" title=\"资料来源\"></a>资料来源</h3><p>本文所有资料均来自于TMU官网中<a href=\"https://www.cit.tum.de/en/cit/home/\">School of Computation,Information and Technology板块</a>，以下简称CIT学院。<br>其中具体信息主要来自于其中的<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/\">学位项目板块</a></p>\n<h3 id=\"硕士学制\"><a href=\"#硕士学制\" class=\"headerlink\" title=\"硕士学制\"></a>硕士学制</h3><p>标准学制2年，以课程为主</p>\n<h3 id=\"在当地工作\"><a href=\"#在当地工作\" class=\"headerlink\" title=\"在当地工作\"></a>在当地工作</h3><p>毕业后到慕尼黑外国人事务局（KVR）把原来的学生居留直接换成“18 个月求职居留”（§ 16b Abs. 4 AufenthG）。<br>在这 18 个月内可以无限制打工（不限天数、不限行业），以维持生活并积累相关经验<br>找到与学位匹配的“合格工作”后，可立即把求职居留转换成“就业居留”（§ 18b AufenthG）。<br>只要这份工作持续 24 个月、并缴纳社保，且德语达到 A1，就能申请德国永久居留。（<a href=\"https://www.community.tum.de/wp-content/uploads/2023/04/TU_Flyer_WorkingInGermany2023_WEB.pdf%EF%BC%89\">https://www.community.tum.de/wp-content/uploads/2023/04/TU_Flyer_WorkingInGermany2023_WEB.pdf）</a></p>\n<p>总的而言，德国还是有一定IT缺口的。</p>\n<h3 id=\"CIT本科专业\"><a href=\"#CIT本科专业\" class=\"headerlink\" title=\"CIT本科专业\"></a>CIT本科专业</h3><p>信息来自于<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/\">学位项目板块</a></p>\n<ul>\n<li>生物信息学</li>\n<li>电气和计算机工程</li>\n<li>电子和数据工程（在新加坡TUM亚洲）</li>\n<li>信息学</li>\n<li>信息学：游戏工程</li>\n<li>信息工程</li>\n<li>信息系统</li>\n<li>数学</li>\n</ul>\n<h3 id=\"本科信息学（Informatics）培养方案（用于检查课程匹配度）\"><a href=\"#本科信息学（Informatics）培养方案（用于检查课程匹配度）\" class=\"headerlink\" title=\"本科信息学（Informatics）培养方案（用于检查课程匹配度）\"></a>本科信息学（Informatics）培养方案（用于检查课程匹配度）</h3><p><a href=\"https://www.cit.tum.de/cit/studium/studiengaenge/bachelor-informatik/studienplan/\">网址</a><br>翻译如下：</p>\n<img src=\"/2025/07/17/TMU%E7%A1%95%E5%A3%AB%E7%94%B3%E8%AF%B7%E8%B0%83%E7%A0%94/%E8%AF%BE%E7%A8%8B.png\" class=\"\">\n<p>对个人而言，这个专业是最契合国内本科计算机科学专业培养方案的。</p>\n<h3 id=\"本科信息工程（Information-Engineering）培养方案\"><a href=\"#本科信息工程（Information-Engineering）培养方案\" class=\"headerlink\" title=\"本科信息工程（Information Engineering）培养方案\"></a>本科信息工程（Information Engineering）培养方案</h3><p><a href=\"https://www.cit.tum.de/fileadmin/w00byx/cit/Studium/Studiengaenge/Bachelor_Information_Engineering_Heilbronn/20221213_Studiengangsdokumentation_BSc_IE_Teil_A_AbgabeSenat__1_.pdf\">官网文件</a><br>主要内容翻译如下：</p>\n<table>\n<thead>\n<tr>\n<th>类别</th>\n<th>学分</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>必修：计算机科学</strong></td>\n<td>90 CP</td>\n<td>信息学核心课程</td>\n</tr>\n<tr>\n<td><strong>必修：数学</strong></td>\n<td>36 CP</td>\n<td>离散结构、线性代数、微积分、概率论</td>\n</tr>\n<tr>\n<td><strong>选修：计算机科学</strong></td>\n<td>12 CP</td>\n<td>从实时系统、虚拟机、密码学等模块中选 2 门</td>\n</tr>\n<tr>\n<td><strong>选修：经济&#x2F;管理</strong></td>\n<td>18 CP</td>\n<td>财务会计、管理科学、物流与生产等 3 门</td>\n</tr>\n<tr>\n<td><strong>选修：跨学科&#x2F;软技能</strong></td>\n<td>9 CP</td>\n<td>伦理、创业、跨文化沟通等</td>\n</tr>\n<tr>\n<td><strong>毕业论文 + 答辩</strong></td>\n<td>15 CP</td>\n<td>第 6 学期完成（12 CP 论文 + 3 CP 答辩）</td>\n</tr>\n<tr>\n<td><strong>必修实践项目</strong></td>\n<td>10 CP</td>\n<td>第 5 学期小组项目（Bachelor Practical Course）</td>\n</tr>\n<tr>\n<td><strong>必修研讨课</strong></td>\n<td>5 CP</td>\n<td>第 3 学期独立完成科研小课题</td>\n</tr>\n<tr>\n<td>添加了一些经管类课程，其他本科专业大同小异，不再赘述。</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h3 id=\"CIT硕士专业\"><a href=\"#CIT硕士专业\" class=\"headerlink\" title=\"CIT硕士专业\"></a>CIT硕士专业</h3><p>信息来自于<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/\">学位项目板块</a><br>生物信息学<br>生物医学计算<br>通信和电子工程<br>计算科学与工程<br>数据工程和分析<br>电气和计算机工程<br>财务和信息管理（与TUM管理学院合作）<br>绿色电子（在新加坡TUM亚洲）<br>信息学<br>信息学：游戏工程<br>信息工程<br>信息系统<br>集成电路设计（在新加坡TUM亚洲）<br>数学<br>数学金融和精算科学<br>数据科学中的数学<br>运筹学中的数学<br>科学与工程中的数学<br>微电子和芯片设计<br>神经工程——精英硕士课程<br>机器人、认知、智能<br>软件工程——精英研究生课程<br>TopMath – 精英项目</p>\n<h3 id=\"计算科学与工程（Computational-Science-and-Engineering）\"><a href=\"#计算科学与工程（Computational-Science-and-Engineering）\" class=\"headerlink\" title=\"计算科学与工程（Computational Science and Engineering）\"></a>计算科学与工程（Computational Science and Engineering）</h3><p>这个项目主要以数学仿真为主。<br><strong>Please note that CSE is not a computer science program, and students who wish to pursue such a program are not encouraged to apply to CSE. If you are interested in computer science or computer engineering, the Informatics Master’s program is for you. The same is true if your primary interest is Robotics or Machine Learning, as these other programs would likely be a better fit for you.</strong><br>对此不感兴趣，故掠过，感兴趣可<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/master-computational-science-engineering/\">访问</a></p>\n<h3 id=\"机器人Robotics-Cognition-Intelligence\"><a href=\"#机器人Robotics-Cognition-Intelligence\" class=\"headerlink\" title=\"机器人Robotics, Cognition, Intelligence\"></a>机器人Robotics, Cognition, Intelligence</h3><p><a href=\"https://www.cit.tum.de/en/cit/studium/studiengaenge/master-robotics-cognition-intelligence/#c2284\">主页</a></p>\n<ul>\n<li><a href=\"https://www.tum.de/studium/studienfinanzierung/studiengebuehren-fuer-studierende-aus-nicht-eu-laendern#c124247\">学费</a><br>6000欧元一学期，一年两学期，大多数工科都一样。</li>\n<li>标准学制：四个学期。</li>\n<li>教学语言：德语和英语（这就意味着需要德语成绩）<br>不符合语言条件，略过。</li>\n</ul>\n<h3 id=\"信息学-Informatics\"><a href=\"#信息学-Informatics\" class=\"headerlink\" title=\"信息学(Informatics)\"></a>信息学(Informatics)</h3><ul>\n<li><a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/master-informatics/\">主页</a></li>\n<li>语言：英语</li>\n<li>申请期限<ul>\n<li>冬季学期：2月1日至5月31日</li>\n<li>夏季学期：9月1日至11月30日</li>\n</ul>\n</li>\n<li>申请流程<a href=\"https://www.tum.de/en/studies/application/master/application-master\">参考</a><ul>\n<li><a href=\"https://campus.tum.de/tumonline/\">注册</a></li>\n<li>能力评估<ul>\n<li>在初始阶段，您在学士学位课程中获得的成绩以及您的书面文件将使用积分系统进行评估。根据累积的积分数量，申请人要么立即被录取，要么被拒绝，要么被邀请参加该部门进行的20分钟的招生面试。在某些情况下，对国际学生进行电话面试。</li>\n<li>面试代表了程序的第二阶段，有助于确定申请人是否能够成功完成所需的学习课程。您可以在您期望的学位课程的学术和考试条例的附录2中找到更多信息。</li>\n</ul>\n</li>\n<li>GRE分数要求：<strong>GRE and GATE<br>Applicants with a Degree from Bangladesh, China, India, Iran or Pakistan have to submit a GRE (General) Test. We have defined required minimum scores, lower scores will not be accepted!<br>The required scores are:<br>Verbal reasoning: (will not be taken into account anymore)<br>Quantitative reasoning: 164<br>Analytical writing: 4.0</strong></li>\n<li>VPD：<br>德语全称：Vorprüfungsdokumentation<br>中文名称：德国大学入学资格预审核证明<br>由 Uni-Assist（德国高校国际合作申请服务中心）出具，用于提前审核非欧盟申请者的学历、成绩和学分是否符合德国大学的入学标准，并把成绩换算成德国计分体系。提前3–6个月向 uni-assist 提交材料，因为：uni-assist处理时间通常为 4–6周，高峰期可能更长；VPD有效期为 1年，但部分大学要求VPD必须在申请截止前不超过6个月内开具</li>\n<li>APS：<br>全称 德国驻华使馆文化处留德人员审核部（Akademische Prüfstelle，简称 APS）审核中国学生的学历、成绩单、学位证等材料是否真实有效。</li>\n<li>命题小论文：科学论文&#x2F;论文应长约1000字，并且必须用英语书写</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\"><a href=\"#总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\" class=\"headerlink\" title=\"总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\"></a>总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。</h3>","excerpt":"","more":"<h3 id=\"资料来源\"><a href=\"#资料来源\" class=\"headerlink\" title=\"资料来源\"></a>资料来源</h3><p>本文所有资料均来自于TMU官网中<a href=\"https://www.cit.tum.de/en/cit/home/\">School of Computation,Information and Technology板块</a>，以下简称CIT学院。<br>其中具体信息主要来自于其中的<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/\">学位项目板块</a></p>\n<h3 id=\"硕士学制\"><a href=\"#硕士学制\" class=\"headerlink\" title=\"硕士学制\"></a>硕士学制</h3><p>标准学制2年，以课程为主</p>\n<h3 id=\"在当地工作\"><a href=\"#在当地工作\" class=\"headerlink\" title=\"在当地工作\"></a>在当地工作</h3><p>毕业后到慕尼黑外国人事务局（KVR）把原来的学生居留直接换成“18 个月求职居留”（§ 16b Abs. 4 AufenthG）。<br>在这 18 个月内可以无限制打工（不限天数、不限行业），以维持生活并积累相关经验<br>找到与学位匹配的“合格工作”后，可立即把求职居留转换成“就业居留”（§ 18b AufenthG）。<br>只要这份工作持续 24 个月、并缴纳社保，且德语达到 A1，就能申请德国永久居留。（<a href=\"https://www.community.tum.de/wp-content/uploads/2023/04/TU_Flyer_WorkingInGermany2023_WEB.pdf%EF%BC%89\">https://www.community.tum.de/wp-content/uploads/2023/04/TU_Flyer_WorkingInGermany2023_WEB.pdf）</a></p>\n<p>总的而言，德国还是有一定IT缺口的。</p>\n<h3 id=\"CIT本科专业\"><a href=\"#CIT本科专业\" class=\"headerlink\" title=\"CIT本科专业\"></a>CIT本科专业</h3><p>信息来自于<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/\">学位项目板块</a></p>\n<ul>\n<li>生物信息学</li>\n<li>电气和计算机工程</li>\n<li>电子和数据工程（在新加坡TUM亚洲）</li>\n<li>信息学</li>\n<li>信息学：游戏工程</li>\n<li>信息工程</li>\n<li>信息系统</li>\n<li>数学</li>\n</ul>\n<h3 id=\"本科信息学（Informatics）培养方案（用于检查课程匹配度）\"><a href=\"#本科信息学（Informatics）培养方案（用于检查课程匹配度）\" class=\"headerlink\" title=\"本科信息学（Informatics）培养方案（用于检查课程匹配度）\"></a>本科信息学（Informatics）培养方案（用于检查课程匹配度）</h3><p><a href=\"https://www.cit.tum.de/cit/studium/studiengaenge/bachelor-informatik/studienplan/\">网址</a><br>翻译如下：</p>\n<img src=\"/2025/07/17/TMU%E7%A1%95%E5%A3%AB%E7%94%B3%E8%AF%B7%E8%B0%83%E7%A0%94/%E8%AF%BE%E7%A8%8B.png\" class=\"\">\n<p>对个人而言，这个专业是最契合国内本科计算机科学专业培养方案的。</p>\n<h3 id=\"本科信息工程（Information-Engineering）培养方案\"><a href=\"#本科信息工程（Information-Engineering）培养方案\" class=\"headerlink\" title=\"本科信息工程（Information Engineering）培养方案\"></a>本科信息工程（Information Engineering）培养方案</h3><p><a href=\"https://www.cit.tum.de/fileadmin/w00byx/cit/Studium/Studiengaenge/Bachelor_Information_Engineering_Heilbronn/20221213_Studiengangsdokumentation_BSc_IE_Teil_A_AbgabeSenat__1_.pdf\">官网文件</a><br>主要内容翻译如下：</p>\n<table>\n<thead>\n<tr>\n<th>类别</th>\n<th>学分</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>必修：计算机科学</strong></td>\n<td>90 CP</td>\n<td>信息学核心课程</td>\n</tr>\n<tr>\n<td><strong>必修：数学</strong></td>\n<td>36 CP</td>\n<td>离散结构、线性代数、微积分、概率论</td>\n</tr>\n<tr>\n<td><strong>选修：计算机科学</strong></td>\n<td>12 CP</td>\n<td>从实时系统、虚拟机、密码学等模块中选 2 门</td>\n</tr>\n<tr>\n<td><strong>选修：经济&#x2F;管理</strong></td>\n<td>18 CP</td>\n<td>财务会计、管理科学、物流与生产等 3 门</td>\n</tr>\n<tr>\n<td><strong>选修：跨学科&#x2F;软技能</strong></td>\n<td>9 CP</td>\n<td>伦理、创业、跨文化沟通等</td>\n</tr>\n<tr>\n<td><strong>毕业论文 + 答辩</strong></td>\n<td>15 CP</td>\n<td>第 6 学期完成（12 CP 论文 + 3 CP 答辩）</td>\n</tr>\n<tr>\n<td><strong>必修实践项目</strong></td>\n<td>10 CP</td>\n<td>第 5 学期小组项目（Bachelor Practical Course）</td>\n</tr>\n<tr>\n<td><strong>必修研讨课</strong></td>\n<td>5 CP</td>\n<td>第 3 学期独立完成科研小课题</td>\n</tr>\n<tr>\n<td>添加了一些经管类课程，其他本科专业大同小异，不再赘述。</td>\n<td></td>\n<td></td>\n</tr>\n</tbody></table>\n<h3 id=\"CIT硕士专业\"><a href=\"#CIT硕士专业\" class=\"headerlink\" title=\"CIT硕士专业\"></a>CIT硕士专业</h3><p>信息来自于<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/\">学位项目板块</a><br>生物信息学<br>生物医学计算<br>通信和电子工程<br>计算科学与工程<br>数据工程和分析<br>电气和计算机工程<br>财务和信息管理（与TUM管理学院合作）<br>绿色电子（在新加坡TUM亚洲）<br>信息学<br>信息学：游戏工程<br>信息工程<br>信息系统<br>集成电路设计（在新加坡TUM亚洲）<br>数学<br>数学金融和精算科学<br>数据科学中的数学<br>运筹学中的数学<br>科学与工程中的数学<br>微电子和芯片设计<br>神经工程——精英硕士课程<br>机器人、认知、智能<br>软件工程——精英研究生课程<br>TopMath – 精英项目</p>\n<h3 id=\"计算科学与工程（Computational-Science-and-Engineering）\"><a href=\"#计算科学与工程（Computational-Science-and-Engineering）\" class=\"headerlink\" title=\"计算科学与工程（Computational Science and Engineering）\"></a>计算科学与工程（Computational Science and Engineering）</h3><p>这个项目主要以数学仿真为主。<br><strong>Please note that CSE is not a computer science program, and students who wish to pursue such a program are not encouraged to apply to CSE. If you are interested in computer science or computer engineering, the Informatics Master’s program is for you. The same is true if your primary interest is Robotics or Machine Learning, as these other programs would likely be a better fit for you.</strong><br>对此不感兴趣，故掠过，感兴趣可<a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/master-computational-science-engineering/\">访问</a></p>\n<h3 id=\"机器人Robotics-Cognition-Intelligence\"><a href=\"#机器人Robotics-Cognition-Intelligence\" class=\"headerlink\" title=\"机器人Robotics, Cognition, Intelligence\"></a>机器人Robotics, Cognition, Intelligence</h3><p><a href=\"https://www.cit.tum.de/en/cit/studium/studiengaenge/master-robotics-cognition-intelligence/#c2284\">主页</a></p>\n<ul>\n<li><a href=\"https://www.tum.de/studium/studienfinanzierung/studiengebuehren-fuer-studierende-aus-nicht-eu-laendern#c124247\">学费</a><br>6000欧元一学期，一年两学期，大多数工科都一样。</li>\n<li>标准学制：四个学期。</li>\n<li>教学语言：德语和英语（这就意味着需要德语成绩）<br>不符合语言条件，略过。</li>\n</ul>\n<h3 id=\"信息学-Informatics\"><a href=\"#信息学-Informatics\" class=\"headerlink\" title=\"信息学(Informatics)\"></a>信息学(Informatics)</h3><ul>\n<li><a href=\"https://www.cit.tum.de/en/cit/studies/degree-programs/master-informatics/\">主页</a></li>\n<li>语言：英语</li>\n<li>申请期限<ul>\n<li>冬季学期：2月1日至5月31日</li>\n<li>夏季学期：9月1日至11月30日</li>\n</ul>\n</li>\n<li>申请流程<a href=\"https://www.tum.de/en/studies/application/master/application-master\">参考</a><ul>\n<li><a href=\"https://campus.tum.de/tumonline/\">注册</a></li>\n<li>能力评估<ul>\n<li>在初始阶段，您在学士学位课程中获得的成绩以及您的书面文件将使用积分系统进行评估。根据累积的积分数量，申请人要么立即被录取，要么被拒绝，要么被邀请参加该部门进行的20分钟的招生面试。在某些情况下，对国际学生进行电话面试。</li>\n<li>面试代表了程序的第二阶段，有助于确定申请人是否能够成功完成所需的学习课程。您可以在您期望的学位课程的学术和考试条例的附录2中找到更多信息。</li>\n</ul>\n</li>\n<li>GRE分数要求：<strong>GRE and GATE<br>Applicants with a Degree from Bangladesh, China, India, Iran or Pakistan have to submit a GRE (General) Test. We have defined required minimum scores, lower scores will not be accepted!<br>The required scores are:<br>Verbal reasoning: (will not be taken into account anymore)<br>Quantitative reasoning: 164<br>Analytical writing: 4.0</strong></li>\n<li>VPD：<br>德语全称：Vorprüfungsdokumentation<br>中文名称：德国大学入学资格预审核证明<br>由 Uni-Assist（德国高校国际合作申请服务中心）出具，用于提前审核非欧盟申请者的学历、成绩和学分是否符合德国大学的入学标准，并把成绩换算成德国计分体系。提前3–6个月向 uni-assist 提交材料，因为：uni-assist处理时间通常为 4–6周，高峰期可能更长；VPD有效期为 1年，但部分大学要求VPD必须在申请截止前不超过6个月内开具</li>\n<li>APS：<br>全称 德国驻华使馆文化处留德人员审核部（Akademische Prüfstelle，简称 APS）审核中国学生的学历、成绩单、学位证等材料是否真实有效。</li>\n<li>命题小论文：科学论文&#x2F;论文应长约1000字，并且必须用英语书写</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\"><a href=\"#总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\" class=\"headerlink\" title=\"总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。\"></a>总而言之：需要托福GRE过关，如果能在第一轮能力评估中通过（学科契合度以及GPA）就会直接录取，否则就要经过面试和考试。</h3>"},{"title":"TUDelt硕士申请调研","date":"2025-08-07T03:06:38.000Z","_content":"### 资料来源\n\nhttps://www.tudelft.nl/en/education/programmes/masters\n\n### 硕士学制\n\n两年\n学费：20,000欧元/年（约16.64万人民币/年）\n\n### 在当地工作\n\n毕业后可申请为期一年的求职签证.\n在求职签证的一年之内，如果顺利找到工作，就可以让雇主帮你申请荷兰工作签证\n欧盟永居申请：在荷兰连续居住超过5年\n\n## MSc Computer & Embedded Systems Engineering\n\nhttps://www.tudelft.nl/en/education/programmes/masters/msc-computer-embedded-systems-engineering\n\n#### 课程匹配：\n\n招生委员会将根据以下关键科目的学习成绩和学习量对申请进行评估：\n数学（微积分、线性代数、数值分析、微分方程）\n概率和统计\n计算机架构\n编程（尤其是C语言）\n\n也非常相关的是：\n数字系统（包括硬件描述语言）\n信号和系统\n信号处理\n操作系统\n\n#### 语言要求：\n\n我们没有设定最低GRE分数，但我们寻找语言推理最低分为154分、定量推理最低分为163分和分析性写作最低分为4.0分的申请人。我们保留拒绝没有这些分数的申请人的权利。\n托福最好100分以上\n\n#### 申请流程\n\n只能申请一个硕士项目\n10月15日开始接收申请\n申请截止日期为1月15日\n\n## MSc Data Science and Artificial Intelligence Technology\n\n#### 课程要求：\n\n申请将由招生委员会根据关键科目的学习成绩和学习负荷进行评估。计算机科学科目的最低学习要求为120个ECTS，其中至少100个ECTS在以下关键科目中获得：\n\n数学和建模：\n微积分、线性代数、概率论和统计学——至少15个ECTS\n软件开发基础知识：\n面向对象编程，软件质量和测试，软件工程方法，编程语言概念，面向对象编程项目，软件项目——至少30个ECTS\n计算机系统：\n计算机组织，计算机网络——至少10个ECTS\n基础计算机科学：\n逻辑、算法和数据结构、算法设计、可计算性——至少15个ECTS\n数据和信息系统：\n机器学习、数据管理、网络和数据库技术——至少15个ECTS\n\n#### 其他同上\n","source":"_posts/TUDelt硕士申请调研.md","raw":"---\ntitle: TUDelt硕士申请调研\ndate: 2025-08-07 11:06:38\ncategories: 随笔杂记\ntags: 留学申请\n---\n### 资料来源\n\nhttps://www.tudelft.nl/en/education/programmes/masters\n\n### 硕士学制\n\n两年\n学费：20,000欧元/年（约16.64万人民币/年）\n\n### 在当地工作\n\n毕业后可申请为期一年的求职签证.\n在求职签证的一年之内，如果顺利找到工作，就可以让雇主帮你申请荷兰工作签证\n欧盟永居申请：在荷兰连续居住超过5年\n\n## MSc Computer & Embedded Systems Engineering\n\nhttps://www.tudelft.nl/en/education/programmes/masters/msc-computer-embedded-systems-engineering\n\n#### 课程匹配：\n\n招生委员会将根据以下关键科目的学习成绩和学习量对申请进行评估：\n数学（微积分、线性代数、数值分析、微分方程）\n概率和统计\n计算机架构\n编程（尤其是C语言）\n\n也非常相关的是：\n数字系统（包括硬件描述语言）\n信号和系统\n信号处理\n操作系统\n\n#### 语言要求：\n\n我们没有设定最低GRE分数，但我们寻找语言推理最低分为154分、定量推理最低分为163分和分析性写作最低分为4.0分的申请人。我们保留拒绝没有这些分数的申请人的权利。\n托福最好100分以上\n\n#### 申请流程\n\n只能申请一个硕士项目\n10月15日开始接收申请\n申请截止日期为1月15日\n\n## MSc Data Science and Artificial Intelligence Technology\n\n#### 课程要求：\n\n申请将由招生委员会根据关键科目的学习成绩和学习负荷进行评估。计算机科学科目的最低学习要求为120个ECTS，其中至少100个ECTS在以下关键科目中获得：\n\n数学和建模：\n微积分、线性代数、概率论和统计学——至少15个ECTS\n软件开发基础知识：\n面向对象编程，软件质量和测试，软件工程方法，编程语言概念，面向对象编程项目，软件项目——至少30个ECTS\n计算机系统：\n计算机组织，计算机网络——至少10个ECTS\n基础计算机科学：\n逻辑、算法和数据结构、算法设计、可计算性——至少15个ECTS\n数据和信息系统：\n机器学习、数据管理、网络和数据库技术——至少15个ECTS\n\n#### 其他同上\n","slug":"TUDelt硕士申请调研","published":1,"updated":"2025-08-08T03:16:47.592Z","comments":1,"layout":"post","photos":[],"_id":"cmf9mvzst0002ad9l90ufhg6v","content":"<h3 id=\"资料来源\"><a href=\"#资料来源\" class=\"headerlink\" title=\"资料来源\"></a>资料来源</h3><p><a href=\"https://www.tudelft.nl/en/education/programmes/masters\">https://www.tudelft.nl/en/education/programmes/masters</a></p>\n<h3 id=\"硕士学制\"><a href=\"#硕士学制\" class=\"headerlink\" title=\"硕士学制\"></a>硕士学制</h3><p>两年<br>学费：20,000欧元&#x2F;年（约16.64万人民币&#x2F;年）</p>\n<h3 id=\"在当地工作\"><a href=\"#在当地工作\" class=\"headerlink\" title=\"在当地工作\"></a>在当地工作</h3><p>毕业后可申请为期一年的求职签证.<br>在求职签证的一年之内，如果顺利找到工作，就可以让雇主帮你申请荷兰工作签证<br>欧盟永居申请：在荷兰连续居住超过5年</p>\n<h2 id=\"MSc-Computer-Embedded-Systems-Engineering\"><a href=\"#MSc-Computer-Embedded-Systems-Engineering\" class=\"headerlink\" title=\"MSc Computer &amp; Embedded Systems Engineering\"></a>MSc Computer &amp; Embedded Systems Engineering</h2><p><a href=\"https://www.tudelft.nl/en/education/programmes/masters/msc-computer-embedded-systems-engineering\">https://www.tudelft.nl/en/education/programmes/masters/msc-computer-embedded-systems-engineering</a></p>\n<h4 id=\"课程匹配：\"><a href=\"#课程匹配：\" class=\"headerlink\" title=\"课程匹配：\"></a>课程匹配：</h4><p>招生委员会将根据以下关键科目的学习成绩和学习量对申请进行评估：<br>数学（微积分、线性代数、数值分析、微分方程）<br>概率和统计<br>计算机架构<br>编程（尤其是C语言）</p>\n<p>也非常相关的是：<br>数字系统（包括硬件描述语言）<br>信号和系统<br>信号处理<br>操作系统</p>\n<h4 id=\"语言要求：\"><a href=\"#语言要求：\" class=\"headerlink\" title=\"语言要求：\"></a>语言要求：</h4><p>我们没有设定最低GRE分数，但我们寻找语言推理最低分为154分、定量推理最低分为163分和分析性写作最低分为4.0分的申请人。我们保留拒绝没有这些分数的申请人的权利。<br>托福最好100分以上</p>\n<h4 id=\"申请流程\"><a href=\"#申请流程\" class=\"headerlink\" title=\"申请流程\"></a>申请流程</h4><p>只能申请一个硕士项目<br>10月15日开始接收申请<br>申请截止日期为1月15日</p>\n<h2 id=\"MSc-Data-Science-and-Artificial-Intelligence-Technology\"><a href=\"#MSc-Data-Science-and-Artificial-Intelligence-Technology\" class=\"headerlink\" title=\"MSc Data Science and Artificial Intelligence Technology\"></a>MSc Data Science and Artificial Intelligence Technology</h2><h4 id=\"课程要求：\"><a href=\"#课程要求：\" class=\"headerlink\" title=\"课程要求：\"></a>课程要求：</h4><p>申请将由招生委员会根据关键科目的学习成绩和学习负荷进行评估。计算机科学科目的最低学习要求为120个ECTS，其中至少100个ECTS在以下关键科目中获得：</p>\n<p>数学和建模：<br>微积分、线性代数、概率论和统计学——至少15个ECTS<br>软件开发基础知识：<br>面向对象编程，软件质量和测试，软件工程方法，编程语言概念，面向对象编程项目，软件项目——至少30个ECTS<br>计算机系统：<br>计算机组织，计算机网络——至少10个ECTS<br>基础计算机科学：<br>逻辑、算法和数据结构、算法设计、可计算性——至少15个ECTS<br>数据和信息系统：<br>机器学习、数据管理、网络和数据库技术——至少15个ECTS</p>\n<h4 id=\"其他同上\"><a href=\"#其他同上\" class=\"headerlink\" title=\"其他同上\"></a>其他同上</h4>","excerpt":"","more":"<h3 id=\"资料来源\"><a href=\"#资料来源\" class=\"headerlink\" title=\"资料来源\"></a>资料来源</h3><p><a href=\"https://www.tudelft.nl/en/education/programmes/masters\">https://www.tudelft.nl/en/education/programmes/masters</a></p>\n<h3 id=\"硕士学制\"><a href=\"#硕士学制\" class=\"headerlink\" title=\"硕士学制\"></a>硕士学制</h3><p>两年<br>学费：20,000欧元&#x2F;年（约16.64万人民币&#x2F;年）</p>\n<h3 id=\"在当地工作\"><a href=\"#在当地工作\" class=\"headerlink\" title=\"在当地工作\"></a>在当地工作</h3><p>毕业后可申请为期一年的求职签证.<br>在求职签证的一年之内，如果顺利找到工作，就可以让雇主帮你申请荷兰工作签证<br>欧盟永居申请：在荷兰连续居住超过5年</p>\n<h2 id=\"MSc-Computer-Embedded-Systems-Engineering\"><a href=\"#MSc-Computer-Embedded-Systems-Engineering\" class=\"headerlink\" title=\"MSc Computer &amp; Embedded Systems Engineering\"></a>MSc Computer &amp; Embedded Systems Engineering</h2><p><a href=\"https://www.tudelft.nl/en/education/programmes/masters/msc-computer-embedded-systems-engineering\">https://www.tudelft.nl/en/education/programmes/masters/msc-computer-embedded-systems-engineering</a></p>\n<h4 id=\"课程匹配：\"><a href=\"#课程匹配：\" class=\"headerlink\" title=\"课程匹配：\"></a>课程匹配：</h4><p>招生委员会将根据以下关键科目的学习成绩和学习量对申请进行评估：<br>数学（微积分、线性代数、数值分析、微分方程）<br>概率和统计<br>计算机架构<br>编程（尤其是C语言）</p>\n<p>也非常相关的是：<br>数字系统（包括硬件描述语言）<br>信号和系统<br>信号处理<br>操作系统</p>\n<h4 id=\"语言要求：\"><a href=\"#语言要求：\" class=\"headerlink\" title=\"语言要求：\"></a>语言要求：</h4><p>我们没有设定最低GRE分数，但我们寻找语言推理最低分为154分、定量推理最低分为163分和分析性写作最低分为4.0分的申请人。我们保留拒绝没有这些分数的申请人的权利。<br>托福最好100分以上</p>\n<h4 id=\"申请流程\"><a href=\"#申请流程\" class=\"headerlink\" title=\"申请流程\"></a>申请流程</h4><p>只能申请一个硕士项目<br>10月15日开始接收申请<br>申请截止日期为1月15日</p>\n<h2 id=\"MSc-Data-Science-and-Artificial-Intelligence-Technology\"><a href=\"#MSc-Data-Science-and-Artificial-Intelligence-Technology\" class=\"headerlink\" title=\"MSc Data Science and Artificial Intelligence Technology\"></a>MSc Data Science and Artificial Intelligence Technology</h2><h4 id=\"课程要求：\"><a href=\"#课程要求：\" class=\"headerlink\" title=\"课程要求：\"></a>课程要求：</h4><p>申请将由招生委员会根据关键科目的学习成绩和学习负荷进行评估。计算机科学科目的最低学习要求为120个ECTS，其中至少100个ECTS在以下关键科目中获得：</p>\n<p>数学和建模：<br>微积分、线性代数、概率论和统计学——至少15个ECTS<br>软件开发基础知识：<br>面向对象编程，软件质量和测试，软件工程方法，编程语言概念，面向对象编程项目，软件项目——至少30个ECTS<br>计算机系统：<br>计算机组织，计算机网络——至少10个ECTS<br>基础计算机科学：<br>逻辑、算法和数据结构、算法设计、可计算性——至少15个ECTS<br>数据和信息系统：<br>机器学习、数据管理、网络和数据库技术——至少15个ECTS</p>\n<h4 id=\"其他同上\"><a href=\"#其他同上\" class=\"headerlink\" title=\"其他同上\"></a>其他同上</h4>"},{"title":"torch的封装层次","date":"2025-07-12T08:21:40.000Z","_content":"> 作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构[github](https://github.com/pytorch/pytorch)，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。\n\n> 0. cuda算子，一般是由核函数组成的.cu和.cpp文件\n> 1. cuda封装，提供参数，调用算子，不进行存储\n> 2. autograd算子，存储求导所需要的临时变量\n> 3. function封装，在上一层的基础上做一些健壮性工作\n> 4. Module封装，把函数封装成类，为的是实现永久性存储\n\ntorch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\n-------------------------------------------------------------------\n\n1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫[cuda算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity)，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）\n\n这个封装层级下你可能会看到这样的调用方式：\n\n```text\nGEMM_cuda.fwd(mat1,mat2)\nGEMM_cuda.bwd(grad_o,mat1,mat2)\n```\n\n这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。\n\nc++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。\n\n但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y=wx，计算w的导数：dy/dw = x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。\n\n---\n\n2: 在cuda算子之上的2级封装是[autograd算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity)，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。\n\n你可能见到这种形式的autograd算子：\n\n```text\nimport torch\nclass TensorModelParallelCrossEntropy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, target, label_smoothing=0.0):\n        # do something\n        ctx.save_for_backward(...)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # do something\n        ... = ctx.saved_tensors\n        return grad_input, None, None\n```\n\n你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。\n\n这种算子可以通过下面这种方式调用：\n\n```text\nce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)\n```\n\n你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。\n\n---\n\n3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：\n\n```text\ndef linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):\n  assert input.is_cuda() and weight.is_cuda()\n  if lora_a is not None and lora_b is not None:\n    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)\n  else:\n    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)\n```\n\n[torch.nn](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity).functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：\n\n```text\nfrom torch.nn.functional import linear,dropout\nlinear(input,weight,bias)\ndropout(input,p=0.1,training=True)\n```\n\n但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。\n\n比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。\n\n---\n\n4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：\n\n```text\nclass Linear(torch.nn.Module):\n  \n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n```\n\n它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：\n\n```text\nlayer = Linear(10,5,bias=False)\nx = torch.randn(2,10)\ny = layer(x)\n```\n\n[Module封装](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity)和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。\n\n到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。\n\n> 作者：真中合欢\n> 链接：https://www.zhihu.com/question/677187311/answer/3780895706\n","source":"_posts/torch的封装层次.md","raw":"---\ntitle: torch的封装层次\ncategories:\n  - 技术实践\ntags:\n  - 转载\n  - pytorch\ndate: 2025-07-12 16:21:40\n---\n> 作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构[github](https://github.com/pytorch/pytorch)，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。\n\n> 0. cuda算子，一般是由核函数组成的.cu和.cpp文件\n> 1. cuda封装，提供参数，调用算子，不进行存储\n> 2. autograd算子，存储求导所需要的临时变量\n> 3. function封装，在上一层的基础上做一些健壮性工作\n> 4. Module封装，把函数封装成类，为的是实现永久性存储\n\ntorch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\n-------------------------------------------------------------------\n\n1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫[cuda算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity)，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）\n\n这个封装层级下你可能会看到这样的调用方式：\n\n```text\nGEMM_cuda.fwd(mat1,mat2)\nGEMM_cuda.bwd(grad_o,mat1,mat2)\n```\n\n这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。\n\nc++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。\n\n但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y=wx，计算w的导数：dy/dw = x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。\n\n---\n\n2: 在cuda算子之上的2级封装是[autograd算子](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity)，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。\n\n你可能见到这种形式的autograd算子：\n\n```text\nimport torch\nclass TensorModelParallelCrossEntropy(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, logits, target, label_smoothing=0.0):\n        # do something\n        ctx.save_for_backward(...)\n        return loss\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        # do something\n        ... = ctx.saved_tensors\n        return grad_input, None, None\n```\n\n你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。\n\n这种算子可以通过下面这种方式调用：\n\n```text\nce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)\n```\n\n你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。\n\n---\n\n3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：\n\n```text\ndef linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):\n  assert input.is_cuda() and weight.is_cuda()\n  if lora_a is not None and lora_b is not None:\n    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)\n  else:\n    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)\n```\n\n[torch.nn](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity).functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：\n\n```text\nfrom torch.nn.functional import linear,dropout\nlinear(input,weight,bias)\ndropout(input,p=0.1,training=True)\n```\n\n但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。\n\n比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。\n\n---\n\n4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：\n\n```text\nclass Linear(torch.nn.Module):\n  \n    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n                 device=None, dtype=None) -> None:\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n        if bias:\n            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n\n    def forward(self, input: Tensor) -> Tensor:\n        return F.linear(input, self.weight, self.bias)\n```\n\n它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：\n\n```text\nlayer = Linear(10,5,bias=False)\nx = torch.randn(2,10)\ny = layer(x)\n```\n\n[Module封装](https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity)和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。\n\n到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。\n\n> 作者：真中合欢\n> 链接：https://www.zhihu.com/question/677187311/answer/3780895706\n","slug":"torch的封装层次","published":1,"updated":"2025-09-07T11:02:46.047Z","comments":1,"layout":"post","photos":[],"_id":"cmf9mvzsv0005ad9lbbo7d19c","content":"<blockquote>\n<p>作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构<a href=\"https://github.com/pytorch/pytorch\">github</a>，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。</p>\n</blockquote>\n<blockquote>\n<ol start=\"0\">\n<li>cuda算子，一般是由核函数组成的.cu和.cpp文件</li>\n<li>cuda封装，提供参数，调用算子，不进行存储</li>\n<li>autograd算子，存储求导所需要的临时变量</li>\n<li>function封装，在上一层的基础上做一些健壮性工作</li>\n<li>Module封装，把函数封装成类，为的是实现永久性存储</li>\n</ol>\n</blockquote>\n<h2 id=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\"><a href=\"#torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\" class=\"headerlink\" title=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\"></a>torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？</h2><p>1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity\">cuda算子</a>，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）</p>\n<p>这个封装层级下你可能会看到这样的调用方式：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GEMM_cuda.fwd(mat1,mat2)</span><br><span class=\"line\">GEMM_cuda.bwd(grad_o,mat1,mat2)</span><br></pre></td></tr></table></figure>\n\n<p>这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。</p>\n<p>c++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。</p>\n<p>但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y&#x3D;wx，计算w的导数：dy&#x2F;dw &#x3D; x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。</p>\n<hr>\n<p>2: 在cuda算子之上的2级封装是<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity\">autograd算子</a>，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。</p>\n<p>你可能见到这种形式的autograd算子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">class TensorModelParallelCrossEntropy(torch.autograd.Function):</span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def forward(ctx, logits, target, label_smoothing=0.0):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ctx.save_for_backward(...)</span><br><span class=\"line\">        return loss</span><br><span class=\"line\"></span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def backward(ctx, grad_output):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ... = ctx.saved_tensors</span><br><span class=\"line\">        return grad_input, None, None</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。</p>\n<p>这种算子可以通过下面这种方式调用：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。</p>\n<hr>\n<p>3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):</span><br><span class=\"line\">  assert input.is_cuda() and weight.is_cuda()</span><br><span class=\"line\">  if lora_a is not None and lora_b is not None:</span><br><span class=\"line\">    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)</span><br><span class=\"line\">  else:</span><br><span class=\"line\">    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity\">torch.nn</a>.functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from torch.nn.functional import linear,dropout</span><br><span class=\"line\">linear(input,weight,bias)</span><br><span class=\"line\">dropout(input,p=0.1,training=True)</span><br></pre></td></tr></table></figure>\n\n<p>但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。</p>\n<p>比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。</p>\n<hr>\n<p>4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Linear(torch.nn.Module):</span><br><span class=\"line\">  </span><br><span class=\"line\">    def __init__(self, in_features: int, out_features: int, bias: bool = True,</span><br><span class=\"line\">                 device=None, dtype=None) -&gt; None:</span><br><span class=\"line\">        factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.in_features = in_features</span><br><span class=\"line\">        self.out_features = out_features</span><br><span class=\"line\">        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))</span><br><span class=\"line\">        if bias:</span><br><span class=\"line\">            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            self.register_parameter(&#x27;bias&#x27;, None)</span><br><span class=\"line\">        self.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    def reset_parameters(self) -&gt; None:</span><br><span class=\"line\">        init.kaiming_uniform_(self.weight, a=math.sqrt(5))</span><br><span class=\"line\">        if self.bias is not None:</span><br><span class=\"line\">            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)</span><br><span class=\"line\">            bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0</span><br><span class=\"line\">            init.uniform_(self.bias, -bound, bound)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, input: Tensor) -&gt; Tensor:</span><br><span class=\"line\">        return F.linear(input, self.weight, self.bias)</span><br></pre></td></tr></table></figure>\n\n<p>它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Linear(10,5,bias=False)</span><br><span class=\"line\">x = torch.randn(2,10)</span><br><span class=\"line\">y = layer(x)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity\">Module封装</a>和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。</p>\n<p>到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。</p>\n<blockquote>\n<p>作者：真中合欢<br>链接：<a href=\"https://www.zhihu.com/question/677187311/answer/3780895706\">https://www.zhihu.com/question/677187311/answer/3780895706</a></p>\n</blockquote>\n","excerpt":"","more":"<blockquote>\n<p>作为初学者,这篇文章对我非常有帮助。如果直接看pytorch的组织结构<a href=\"https://github.com/pytorch/pytorch\">github</a>，总是一头雾水，这篇文章提供了一个帮助，去了解pytorch是怎样一步步组织起来的。</p>\n</blockquote>\n<blockquote>\n<ol start=\"0\">\n<li>cuda算子，一般是由核函数组成的.cu和.cpp文件</li>\n<li>cuda封装，提供参数，调用算子，不进行存储</li>\n<li>autograd算子，存储求导所需要的临时变量</li>\n<li>function封装，在上一层的基础上做一些健壮性工作</li>\n<li>Module封装，把函数封装成类，为的是实现永久性存储</li>\n</ol>\n</blockquote>\n<h2 id=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\"><a href=\"#torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\" class=\"headerlink\" title=\"torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？\"></a>torch中同一个功能，不同层级的封装有什么用？我应该用什么层级的封装？</h2><p>1:首先我们说在torch中你能看到的最基础封装是cuda封装，或者叫<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=cuda%E7%AE%97%E5%AD%90&zhida_source=entity\">cuda算子</a>，我们算它是1级封装。（方便起见我们这里忽略triton等其他方式实现的算子）</p>\n<p>这个封装层级下你可能会看到这样的调用方式：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">GEMM_cuda.fwd(mat1,mat2)</span><br><span class=\"line\">GEMM_cuda.bwd(grad_o,mat1,mat2)</span><br></pre></td></tr></table></figure>\n\n<p>这种封装是python封装的最底层，它是在调用更底层的c++算子，实现矩阵运算的正向传播和反向传播。</p>\n<p>c++这个层级的算子只负责计算，计算之后相应的内存空间就销毁，不会存储任何东西。</p>\n<p>但我们知道torch是支持自动求导的，自动求导是依据链式法则实现的。一个简单的乘法：y&#x3D;wx，计算w的导数：dy&#x2F;dw &#x3D; x，你会发现w的导数就是x，那么在我们计算w的导数时，就需要知道x，而x是正向传播时传递过来的，因此我们需要在正向传播时存下这个x。上面又说了cuda算子只负责计算，不负责存储，那么我们就需要更高一级的封装，来存储这些求导所使用的临时变量。</p>\n<hr>\n<p>2: 在cuda算子之上的2级封装是<a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=autograd%E7%AE%97%E5%AD%90&zhida_source=entity\">autograd算子</a>，它是通过继承torch.autograd.Function来实现的。这个层级的封装就是为了存储求导所需要的临时变量。从这一个层级开始就都是python代码了。</p>\n<p>你可能见到这种形式的autograd算子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">import torch</span><br><span class=\"line\">class TensorModelParallelCrossEntropy(torch.autograd.Function):</span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def forward(ctx, logits, target, label_smoothing=0.0):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ctx.save_for_backward(...)</span><br><span class=\"line\">        return loss</span><br><span class=\"line\"></span><br><span class=\"line\">    @staticmethod</span><br><span class=\"line\">    def backward(ctx, grad_output):</span><br><span class=\"line\">        # do something</span><br><span class=\"line\">        ... = ctx.saved_tensors</span><br><span class=\"line\">        return grad_input, None, None</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到这种算子中会存在一个save_for_backward函数，专门存储反向传播需要的临时变量。</p>\n<p>这种算子可以通过下面这种方式调用：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ce_loss = TensorModelParallelCrossEntropy.apply(logits,labels)</span><br></pre></td></tr></table></figure>\n\n<p>你可以看到调用这种算子并不是通过使用它的forward或者backward函数，而是使用apply函数。这里torch会进行一些封装，例如调用apply后，会用forward进行计算，并将backward添加到tensor的grad_fn属性计算图中，求导时自动调用。会在使用了torch.no_grad()上下文，不需要求导时，自动抛弃掉save_for_backward存储的张量。但是这种层级用的也不是太常见，首先观察forward函数的输入参数和backward的输出参数。backward函数返回的梯度数量必须和forward输入参数的数量相同，但是可以用None占位。比如target是标签，label_smoothing是超参，不可学习，不需要导数，这里就会用None占位。因此当你需要某一个功能的时候，需要严格的选择你需要的autograd算子，达到最佳的计算效率，不需要计算的东西不要算。</p>\n<hr>\n<p>3: 接下来就是第3级function封装。function封装的作用就是增加autograd算子的灵活性和健壮性，比如做一些异常检测，默认值填补，找到合适的autograd算子分发等等，比如这样：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">def linear_with_grad_accumulation_and_async_allreduce(input,weight,bias,lora_a=None,lora_b=None):</span><br><span class=\"line\">  assert input.is_cuda() and weight.is_cuda()</span><br><span class=\"line\">  if lora_a is not None and lora_b is not None:</span><br><span class=\"line\">    return LoraLinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias,lora_a,lora_b)</span><br><span class=\"line\">  else:</span><br><span class=\"line\">    return LinearWithGradAccumulationAndAsyncCommunication.apply(input,weight,bias)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=torch.nn&zhida_source=entity\">torch.nn</a>.functional里面的函数就是这一级封装，这一级的函数对于大部分的人来说已经可以拿来用了，比如：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">from torch.nn.functional import linear,dropout</span><br><span class=\"line\">linear(input,weight,bias)</span><br><span class=\"line\">dropout(input,p=0.1,training=True)</span><br></pre></td></tr></table></figure>\n\n<p>但是这个层级的封装依旧只会存储正、反向传播的临时变量，并不会存储一些持久化存在的变量。</p>\n<p>比如看到linear函数，它的输入有input、weight、bias，其中input是一个临时变量，你的模型输入数据了，input就有，不输入就没有，输入不同的值input也不同。但是weight和bias是模型定义的时候就存在的，与你是否正向传播无关，也不会随着你输入input的值不同而改变。看到dropout函数，丢弃率p和模型当前是处于训练状态还是推理状态，也不是一个会每次都变的值。所以我们还需要一层封装来存储这些不会临时改变的东西。</p>\n<hr>\n<p>4:这第4级封装就是torch的Module级别封装，也就是题主题目中提到的“用类实现”。类似这个样子：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">class Linear(torch.nn.Module):</span><br><span class=\"line\">  </span><br><span class=\"line\">    def __init__(self, in_features: int, out_features: int, bias: bool = True,</span><br><span class=\"line\">                 device=None, dtype=None) -&gt; None:</span><br><span class=\"line\">        factory_kwargs = &#123;&#x27;device&#x27;: device, &#x27;dtype&#x27;: dtype&#125;</span><br><span class=\"line\">        super().__init__()</span><br><span class=\"line\">        self.in_features = in_features</span><br><span class=\"line\">        self.out_features = out_features</span><br><span class=\"line\">        self.weight = torch.nn.Parameter(torch.empty((out_features, in_features), **factory_kwargs))</span><br><span class=\"line\">        if bias:</span><br><span class=\"line\">            self.bias = torch.nn.Parameter(torch.empty(out_features, **factory_kwargs))</span><br><span class=\"line\">        else:</span><br><span class=\"line\">            self.register_parameter(&#x27;bias&#x27;, None)</span><br><span class=\"line\">        self.reset_parameters()</span><br><span class=\"line\"></span><br><span class=\"line\">    def reset_parameters(self) -&gt; None:</span><br><span class=\"line\">        init.kaiming_uniform_(self.weight, a=math.sqrt(5))</span><br><span class=\"line\">        if self.bias is not None:</span><br><span class=\"line\">            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)</span><br><span class=\"line\">            bound = 1 / math.sqrt(fan_in) if fan_in &gt; 0 else 0</span><br><span class=\"line\">            init.uniform_(self.bias, -bound, bound)</span><br><span class=\"line\"></span><br><span class=\"line\">    def forward(self, input: Tensor) -&gt; Tensor:</span><br><span class=\"line\">        return F.linear(input, self.weight, self.bias)</span><br></pre></td></tr></table></figure>\n\n<p>它会帮你定义持久存储的参数weight和bias，会帮你自动初始化这些参数，比如使用kaiming初始化。在你调用这个类创建的实例时，它会调用这个类的forward函数：</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Linear(10,5,bias=False)</span><br><span class=\"line\">x = torch.randn(2,10)</span><br><span class=\"line\">y = layer(x)</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://zhida.zhihu.com/search?content_id=691781227&content_type=Answer&match_order=1&q=Module%E5%B0%81%E8%A3%85&zhida_source=entity\">Module封装</a>和autograd封装一样，调用和定义的函数名是不同的，同样是因为torch后台帮你做了一些操作，比如判断类是否有某个属性，判断类多重继承时应该调用谁的函数，给正反向传播的输入和输出添加一些钩子函数等。</p>\n<p>到这里题主的问题，为什么要用类，为什么不用函数就已经很明确了。不想管理持久化的变量，就用Module封装，想要手动管理，就用function封装。想要自定义正反向传播的计算方法，就去写autograd算子，想炸裂提效，做算子融合，就去写cuda或者triton算子。</p>\n<blockquote>\n<p>作者：真中合欢<br>链接：<a href=\"https://www.zhihu.com/question/677187311/answer/3780895706\">https://www.zhihu.com/question/677187311/answer/3780895706</a></p>\n</blockquote>\n"},{"title":"二十年前的相机","date":"2025-07-13T02:13:40.000Z","_content":"# Canon PowerShot G5\n\n出于“还能用的东西统统留着”的原则，经常能从家里的角落找到老古董，感觉时间仿佛静止了一般。\n这台佳能相机于2003年上市，04年购入，据回忆当时的价格大约5000元。我对相机技术并不是很了解，不过当年想必也是比较先进的机型。在Amazon上可以找到当时的评价：\n\n> I bought this camera to replace a 2mp camera that allowed no control over aperture and speed and to give me the greater resolution that would allow larger than 8x10 prints. I was very pleased with the menu structure and layout of the controls but I was very surprised to see the lens barrel visible through the optical viewfinder, obscuring the lower left part of a shot unless the camera is zoomed in. The lens is outstanding, and shots taken at ISO 50 are wonderfully free of noise even in low light. What killed the camera for me is the autofocus. For some reason it often has a hard time finding focus. In several situations from macro to landscape and at various light levels the autofocus would hunt for seconds and then occasionally would give up and lose focus entirely. The conditions mentioned in the manual that might cause difficulty with focus were not present. Of course, I could focus manually but that can be a tedious task using the LCD display (and not just with this camera), especially in bright light. The camera has a good feel and is easy to use but I finally gave it up from the frustration with focus. I bought an Olympus C-5050 which is not quite so easy to use but the autofocus is rapid and reliable. I frequently see a shot where I must get the camera out and on it quickly and the G5 couldn't be counted on.\n\n---\n\n这则评价发布于2003年，对焦速度慢，经常对焦错误——到这确实是非常中肯的评价。不过放在今天，复古的自动对焦过程、伸缩镜头发出的机械声反而让人狂喜。译文如下：\n\n> 我购买这台相机是为了替换一台200万像素的相机，那台相机无法调节光圈和快门速度，而这台相机则能提供更高的分辨率，支持打印大于8x10英寸的相片。我对相机的菜单结构和控制布局非常满意，但令我惊讶的是，通过光学取景器可以看到镜头筒，这会遮挡画面左下角的部分，除非将相机进行变焦。镜头表现出色，即使在低光环境下，ISO 50拍摄的画面也几乎没有噪点。但自动对焦功能让我对这台相机失去了兴趣。不知何故，它经常难以找到对焦点。在从微距到风景的多种拍摄场景和不同光线条件下，自动对焦系统会搜索数秒，偶尔甚至会放弃对焦并完全失焦。而说明书中提到的可能导致对焦困难的条件并未出现。当然，我可以手动对焦，但使用LCD屏幕进行手动对焦（不仅限于这台相机）在强光下会非常繁琐。这台相机手感良好且易于使用，但我最终因对焦问题而放弃了它。我购买了奥林巴斯C-5050，虽然它使用起来没有那么方便，但自动对焦速度快且可靠。我经常遇到需要迅速拿出相机并快速拍摄的场景，而G5无法满足这一需求。\n\n最后一次使用大概是在12或者13年的儿童节（里面的cf卡中竟然还有没导出的相片），随后就一直放在地下室。中间地下室遭过一次贼，居然幸免遇难。今天找到，箱包完整，充电后一切功能正常，质量实在过硬。\n\n{% asset_img \"包装.jpg\"%}\n照片由G5拍摄，500万像素还是相当清晰。盒子八角尖尖，不错。\n\n{% asset_img \"内部.jpg\"%}\n打开盒子，还能闻到十年前的~~灰尘~~空气。说明书完整，保修单还一次都没用过。\n\n{% asset_img \"相机.jpg\"%}\n给相机来一张大头照，握把已经老化的黏糊糊了，有点可惜。\n本来想展示一下细节，发现已经有人做出来了[测评](https://www.bilibili.com/video/BV1He4y1h7Ee/?spm_id_from=333.337.search-card.all.click&vd_source=afec0ec631ec084a74fcdd3c2f49a6ab)\n\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=560256914&bvid=BV1He4y1h7Ee&cid=826868209&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n照相功能放在今天来看依然不错，带着可爱的色彩，只是视频的情况就不敢恭维了——这是考古人员再cf卡中读到的一段视频：\n\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=114846385315535&bvid=BV1hUuPzBE2Q&cid=31019304770&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n其实这个节目我是有上去表演的，节目的内容就是大家跟着哆啦A梦的音乐跳舞。参加表演的大多是女孩子，这项节目大概只有我们三个男生参加，其中一个是我的好朋友，他早熟又擅于表现，所以总是很受欢迎；另一个同学聪明沉稳，他是我们中最像大人的一个。本人属实是其中最笨拙的，彩排时就跟在上述第二个同学后面有样学样，表演时毫不协调只能滥竽充数了。\n想到这，不禁困惑当时为什么参加这个活动，也许是为了配合好兄弟的表演罢。之后再上中学和大学，却也没有参加过跳舞这样的节目了。\n\n最后怀着激动的心情用这台相机照一张像，完成与小时候的交接：\n{% asset_img \"照片.jpg\" %}\n~~对焦没有对到镜子里的虚像反而对到镜框上去了啊啊啊啊啊啊~~\n","source":"_posts/二十年前的相机.md","raw":"---\ntitle: 二十年前的相机\ndate: 2025-07-13 10:13:40\ncategories:\n  - 随笔杂记  \ntags:\n  - 摄影\n---\n# Canon PowerShot G5\n\n出于“还能用的东西统统留着”的原则，经常能从家里的角落找到老古董，感觉时间仿佛静止了一般。\n这台佳能相机于2003年上市，04年购入，据回忆当时的价格大约5000元。我对相机技术并不是很了解，不过当年想必也是比较先进的机型。在Amazon上可以找到当时的评价：\n\n> I bought this camera to replace a 2mp camera that allowed no control over aperture and speed and to give me the greater resolution that would allow larger than 8x10 prints. I was very pleased with the menu structure and layout of the controls but I was very surprised to see the lens barrel visible through the optical viewfinder, obscuring the lower left part of a shot unless the camera is zoomed in. The lens is outstanding, and shots taken at ISO 50 are wonderfully free of noise even in low light. What killed the camera for me is the autofocus. For some reason it often has a hard time finding focus. In several situations from macro to landscape and at various light levels the autofocus would hunt for seconds and then occasionally would give up and lose focus entirely. The conditions mentioned in the manual that might cause difficulty with focus were not present. Of course, I could focus manually but that can be a tedious task using the LCD display (and not just with this camera), especially in bright light. The camera has a good feel and is easy to use but I finally gave it up from the frustration with focus. I bought an Olympus C-5050 which is not quite so easy to use but the autofocus is rapid and reliable. I frequently see a shot where I must get the camera out and on it quickly and the G5 couldn't be counted on.\n\n---\n\n这则评价发布于2003年，对焦速度慢，经常对焦错误——到这确实是非常中肯的评价。不过放在今天，复古的自动对焦过程、伸缩镜头发出的机械声反而让人狂喜。译文如下：\n\n> 我购买这台相机是为了替换一台200万像素的相机，那台相机无法调节光圈和快门速度，而这台相机则能提供更高的分辨率，支持打印大于8x10英寸的相片。我对相机的菜单结构和控制布局非常满意，但令我惊讶的是，通过光学取景器可以看到镜头筒，这会遮挡画面左下角的部分，除非将相机进行变焦。镜头表现出色，即使在低光环境下，ISO 50拍摄的画面也几乎没有噪点。但自动对焦功能让我对这台相机失去了兴趣。不知何故，它经常难以找到对焦点。在从微距到风景的多种拍摄场景和不同光线条件下，自动对焦系统会搜索数秒，偶尔甚至会放弃对焦并完全失焦。而说明书中提到的可能导致对焦困难的条件并未出现。当然，我可以手动对焦，但使用LCD屏幕进行手动对焦（不仅限于这台相机）在强光下会非常繁琐。这台相机手感良好且易于使用，但我最终因对焦问题而放弃了它。我购买了奥林巴斯C-5050，虽然它使用起来没有那么方便，但自动对焦速度快且可靠。我经常遇到需要迅速拿出相机并快速拍摄的场景，而G5无法满足这一需求。\n\n最后一次使用大概是在12或者13年的儿童节（里面的cf卡中竟然还有没导出的相片），随后就一直放在地下室。中间地下室遭过一次贼，居然幸免遇难。今天找到，箱包完整，充电后一切功能正常，质量实在过硬。\n\n{% asset_img \"包装.jpg\"%}\n照片由G5拍摄，500万像素还是相当清晰。盒子八角尖尖，不错。\n\n{% asset_img \"内部.jpg\"%}\n打开盒子，还能闻到十年前的~~灰尘~~空气。说明书完整，保修单还一次都没用过。\n\n{% asset_img \"相机.jpg\"%}\n给相机来一张大头照，握把已经老化的黏糊糊了，有点可惜。\n本来想展示一下细节，发现已经有人做出来了[测评](https://www.bilibili.com/video/BV1He4y1h7Ee/?spm_id_from=333.337.search-card.all.click&vd_source=afec0ec631ec084a74fcdd3c2f49a6ab)\n\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=560256914&bvid=BV1He4y1h7Ee&cid=826868209&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n照相功能放在今天来看依然不错，带着可爱的色彩，只是视频的情况就不敢恭维了——这是考古人员再cf卡中读到的一段视频：\n\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=114846385315535&bvid=BV1hUuPzBE2Q&cid=31019304770&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n其实这个节目我是有上去表演的，节目的内容就是大家跟着哆啦A梦的音乐跳舞。参加表演的大多是女孩子，这项节目大概只有我们三个男生参加，其中一个是我的好朋友，他早熟又擅于表现，所以总是很受欢迎；另一个同学聪明沉稳，他是我们中最像大人的一个。本人属实是其中最笨拙的，彩排时就跟在上述第二个同学后面有样学样，表演时毫不协调只能滥竽充数了。\n想到这，不禁困惑当时为什么参加这个活动，也许是为了配合好兄弟的表演罢。之后再上中学和大学，却也没有参加过跳舞这样的节目了。\n\n最后怀着激动的心情用这台相机照一张像，完成与小时候的交接：\n{% asset_img \"照片.jpg\" %}\n~~对焦没有对到镜子里的虚像反而对到镜框上去了啊啊啊啊啊啊~~\n","slug":"二十年前的相机","published":1,"updated":"2025-09-07T11:15:16.906Z","comments":1,"layout":"post","photos":[],"_id":"cmf9mvzsv0006ad9l65l0eowv","content":"<h1 id=\"Canon-PowerShot-G5\"><a href=\"#Canon-PowerShot-G5\" class=\"headerlink\" title=\"Canon PowerShot G5\"></a>Canon PowerShot G5</h1><p>出于“还能用的东西统统留着”的原则，经常能从家里的角落找到老古董，感觉时间仿佛静止了一般。<br>这台佳能相机于2003年上市，04年购入，据回忆当时的价格大约5000元。我对相机技术并不是很了解，不过当年想必也是比较先进的机型。在Amazon上可以找到当时的评价：</p>\n<blockquote>\n<p>I bought this camera to replace a 2mp camera that allowed no control over aperture and speed and to give me the greater resolution that would allow larger than 8x10 prints. I was very pleased with the menu structure and layout of the controls but I was very surprised to see the lens barrel visible through the optical viewfinder, obscuring the lower left part of a shot unless the camera is zoomed in. The lens is outstanding, and shots taken at ISO 50 are wonderfully free of noise even in low light. What killed the camera for me is the autofocus. For some reason it often has a hard time finding focus. In several situations from macro to landscape and at various light levels the autofocus would hunt for seconds and then occasionally would give up and lose focus entirely. The conditions mentioned in the manual that might cause difficulty with focus were not present. Of course, I could focus manually but that can be a tedious task using the LCD display (and not just with this camera), especially in bright light. The camera has a good feel and is easy to use but I finally gave it up from the frustration with focus. I bought an Olympus C-5050 which is not quite so easy to use but the autofocus is rapid and reliable. I frequently see a shot where I must get the camera out and on it quickly and the G5 couldn’t be counted on.</p>\n</blockquote>\n<hr>\n<p>这则评价发布于2003年，对焦速度慢，经常对焦错误——到这确实是非常中肯的评价。不过放在今天，复古的自动对焦过程、伸缩镜头发出的机械声反而让人狂喜。译文如下：</p>\n<blockquote>\n<p>我购买这台相机是为了替换一台200万像素的相机，那台相机无法调节光圈和快门速度，而这台相机则能提供更高的分辨率，支持打印大于8x10英寸的相片。我对相机的菜单结构和控制布局非常满意，但令我惊讶的是，通过光学取景器可以看到镜头筒，这会遮挡画面左下角的部分，除非将相机进行变焦。镜头表现出色，即使在低光环境下，ISO 50拍摄的画面也几乎没有噪点。但自动对焦功能让我对这台相机失去了兴趣。不知何故，它经常难以找到对焦点。在从微距到风景的多种拍摄场景和不同光线条件下，自动对焦系统会搜索数秒，偶尔甚至会放弃对焦并完全失焦。而说明书中提到的可能导致对焦困难的条件并未出现。当然，我可以手动对焦，但使用LCD屏幕进行手动对焦（不仅限于这台相机）在强光下会非常繁琐。这台相机手感良好且易于使用，但我最终因对焦问题而放弃了它。我购买了奥林巴斯C-5050，虽然它使用起来没有那么方便，但自动对焦速度快且可靠。我经常遇到需要迅速拿出相机并快速拍摄的场景，而G5无法满足这一需求。</p>\n</blockquote>\n<p>最后一次使用大概是在12或者13年的儿童节（里面的cf卡中竟然还有没导出的相片），随后就一直放在地下室。中间地下室遭过一次贼，居然幸免遇难。今天找到，箱包完整，充电后一切功能正常，质量实在过硬。</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E5%8C%85%E8%A3%85.jpg\" class=\"\">\n<p>照片由G5拍摄，500万像素还是相当清晰。盒子八角尖尖，不错。</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E5%86%85%E9%83%A8.jpg\" class=\"\">\n<p>打开盒子，还能闻到十年前的<del>灰尘</del>空气。说明书完整，保修单还一次都没用过。</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E7%9B%B8%E6%9C%BA.jpg\" class=\"\">\n<p>给相机来一张大头照，握把已经老化的黏糊糊了，有点可惜。<br>本来想展示一下细节，发现已经有人做出来了<a href=\"https://www.bilibili.com/video/BV1He4y1h7Ee/?spm_id_from=333.337.search-card.all.click&vd_source=afec0ec631ec084a74fcdd3c2f49a6ab\">测评</a></p>\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=560256914&bvid=BV1He4y1h7Ee&cid=826868209&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n<p>照相功能放在今天来看依然不错，带着可爱的色彩，只是视频的情况就不敢恭维了——这是考古人员再cf卡中读到的一段视频：</p>\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=114846385315535&bvid=BV1hUuPzBE2Q&cid=31019304770&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n<p>其实这个节目我是有上去表演的，节目的内容就是大家跟着哆啦A梦的音乐跳舞。参加表演的大多是女孩子，这项节目大概只有我们三个男生参加，其中一个是我的好朋友，他早熟又擅于表现，所以总是很受欢迎；另一个同学聪明沉稳，他是我们中最像大人的一个。本人属实是其中最笨拙的，彩排时就跟在上述第二个同学后面有样学样，表演时毫不协调只能滥竽充数了。<br>想到这，不禁困惑当时为什么参加这个活动，也许是为了配合好兄弟的表演罢。之后再上中学和大学，却也没有参加过跳舞这样的节目了。</p>\n<p>最后怀着激动的心情用这台相机照一张像，完成与小时候的交接：</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E7%85%A7%E7%89%87.jpg\" class=\"\">\n<p><del>对焦没有对到镜子里的虚像反而对到镜框上去了啊啊啊啊啊啊</del></p>\n","excerpt":"","more":"<h1 id=\"Canon-PowerShot-G5\"><a href=\"#Canon-PowerShot-G5\" class=\"headerlink\" title=\"Canon PowerShot G5\"></a>Canon PowerShot G5</h1><p>出于“还能用的东西统统留着”的原则，经常能从家里的角落找到老古董，感觉时间仿佛静止了一般。<br>这台佳能相机于2003年上市，04年购入，据回忆当时的价格大约5000元。我对相机技术并不是很了解，不过当年想必也是比较先进的机型。在Amazon上可以找到当时的评价：</p>\n<blockquote>\n<p>I bought this camera to replace a 2mp camera that allowed no control over aperture and speed and to give me the greater resolution that would allow larger than 8x10 prints. I was very pleased with the menu structure and layout of the controls but I was very surprised to see the lens barrel visible through the optical viewfinder, obscuring the lower left part of a shot unless the camera is zoomed in. The lens is outstanding, and shots taken at ISO 50 are wonderfully free of noise even in low light. What killed the camera for me is the autofocus. For some reason it often has a hard time finding focus. In several situations from macro to landscape and at various light levels the autofocus would hunt for seconds and then occasionally would give up and lose focus entirely. The conditions mentioned in the manual that might cause difficulty with focus were not present. Of course, I could focus manually but that can be a tedious task using the LCD display (and not just with this camera), especially in bright light. The camera has a good feel and is easy to use but I finally gave it up from the frustration with focus. I bought an Olympus C-5050 which is not quite so easy to use but the autofocus is rapid and reliable. I frequently see a shot where I must get the camera out and on it quickly and the G5 couldn’t be counted on.</p>\n</blockquote>\n<hr>\n<p>这则评价发布于2003年，对焦速度慢，经常对焦错误——到这确实是非常中肯的评价。不过放在今天，复古的自动对焦过程、伸缩镜头发出的机械声反而让人狂喜。译文如下：</p>\n<blockquote>\n<p>我购买这台相机是为了替换一台200万像素的相机，那台相机无法调节光圈和快门速度，而这台相机则能提供更高的分辨率，支持打印大于8x10英寸的相片。我对相机的菜单结构和控制布局非常满意，但令我惊讶的是，通过光学取景器可以看到镜头筒，这会遮挡画面左下角的部分，除非将相机进行变焦。镜头表现出色，即使在低光环境下，ISO 50拍摄的画面也几乎没有噪点。但自动对焦功能让我对这台相机失去了兴趣。不知何故，它经常难以找到对焦点。在从微距到风景的多种拍摄场景和不同光线条件下，自动对焦系统会搜索数秒，偶尔甚至会放弃对焦并完全失焦。而说明书中提到的可能导致对焦困难的条件并未出现。当然，我可以手动对焦，但使用LCD屏幕进行手动对焦（不仅限于这台相机）在强光下会非常繁琐。这台相机手感良好且易于使用，但我最终因对焦问题而放弃了它。我购买了奥林巴斯C-5050，虽然它使用起来没有那么方便，但自动对焦速度快且可靠。我经常遇到需要迅速拿出相机并快速拍摄的场景，而G5无法满足这一需求。</p>\n</blockquote>\n<p>最后一次使用大概是在12或者13年的儿童节（里面的cf卡中竟然还有没导出的相片），随后就一直放在地下室。中间地下室遭过一次贼，居然幸免遇难。今天找到，箱包完整，充电后一切功能正常，质量实在过硬。</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E5%8C%85%E8%A3%85.jpg\" class=\"\">\n<p>照片由G5拍摄，500万像素还是相当清晰。盒子八角尖尖，不错。</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E5%86%85%E9%83%A8.jpg\" class=\"\">\n<p>打开盒子，还能闻到十年前的<del>灰尘</del>空气。说明书完整，保修单还一次都没用过。</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E7%9B%B8%E6%9C%BA.jpg\" class=\"\">\n<p>给相机来一张大头照，握把已经老化的黏糊糊了，有点可惜。<br>本来想展示一下细节，发现已经有人做出来了<a href=\"https://www.bilibili.com/video/BV1He4y1h7Ee/?spm_id_from=333.337.search-card.all.click&vd_source=afec0ec631ec084a74fcdd3c2f49a6ab\">测评</a></p>\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=560256914&bvid=BV1He4y1h7Ee&cid=826868209&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n<p>照相功能放在今天来看依然不错，带着可爱的色彩，只是视频的情况就不敢恭维了——这是考古人员再cf卡中读到的一段视频：</p>\n<iframe src=\"https://player.bilibili.com/player.html?isOutside=true&aid=114846385315535&bvid=BV1hUuPzBE2Q&cid=31019304770&p=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" autoplay=false></iframe>\n\n<p>其实这个节目我是有上去表演的，节目的内容就是大家跟着哆啦A梦的音乐跳舞。参加表演的大多是女孩子，这项节目大概只有我们三个男生参加，其中一个是我的好朋友，他早熟又擅于表现，所以总是很受欢迎；另一个同学聪明沉稳，他是我们中最像大人的一个。本人属实是其中最笨拙的，彩排时就跟在上述第二个同学后面有样学样，表演时毫不协调只能滥竽充数了。<br>想到这，不禁困惑当时为什么参加这个活动，也许是为了配合好兄弟的表演罢。之后再上中学和大学，却也没有参加过跳舞这样的节目了。</p>\n<p>最后怀着激动的心情用这台相机照一张像，完成与小时候的交接：</p>\n<img src=\"/2025/07/13/%E4%BA%8C%E5%8D%81%E5%B9%B4%E5%89%8D%E7%9A%84%E7%9B%B8%E6%9C%BA/%E7%85%A7%E7%89%87.jpg\" class=\"\">\n<p><del>对焦没有对到镜子里的虚像反而对到镜框上去了啊啊啊啊啊啊</del></p>\n"}],"PostAsset":[{"_id":"source/_posts/TMU硕士申请调研/课程.png","post":"cmf9mvzss0001ad9l6adhdu5p","slug":"课程.png","modified":1,"renderable":0},{"_id":"source/_posts/二十年前的相机/内部.jpg","post":"cmf9mvzsv0006ad9l65l0eowv","slug":"内部.jpg","modified":1,"renderable":0},{"_id":"source/_posts/二十年前的相机/包装.jpg","post":"cmf9mvzsv0006ad9l65l0eowv","slug":"包装.jpg","modified":1,"renderable":0},{"_id":"source/_posts/二十年前的相机/照片.jpg","post":"cmf9mvzsv0006ad9l65l0eowv","slug":"照片.jpg","modified":1,"renderable":0},{"_id":"source/_posts/二十年前的相机/相机.jpg","post":"cmf9mvzsv0006ad9l65l0eowv","slug":"相机.jpg","modified":1,"renderable":0}],"PostCategory":[{"post_id":"cmf9mvzsv0006ad9l65l0eowv","category_id":"cmf9mvzsu0003ad9l3cmyhcus","_id":"cmf9mvzsx000aad9l8nvb0qq1"},{"post_id":"cmf9mvzss0001ad9l6adhdu5p","category_id":"cmf9mvzsu0003ad9l3cmyhcus","_id":"cmf9mvzsx000ead9lfwef914z"},{"post_id":"cmf9mvzst0002ad9l90ufhg6v","category_id":"cmf9mvzsu0003ad9l3cmyhcus","_id":"cmf9mvzsx000fad9lgf2x2er2"},{"post_id":"cmf9mvzsv0005ad9lbbo7d19c","category_id":"cmf9mvzsx000bad9l3vd82zc0","_id":"cmf9mvzsx000had9l32zjh19k"}],"PostTag":[{"post_id":"cmf9mvzss0001ad9l6adhdu5p","tag_id":"cmf9mvzsv0004ad9lg3x17vpo","_id":"cmf9mvzsw0009ad9l9eeaagd5"},{"post_id":"cmf9mvzst0002ad9l90ufhg6v","tag_id":"cmf9mvzsv0004ad9lg3x17vpo","_id":"cmf9mvzsx000dad9l34y5hfab"},{"post_id":"cmf9mvzsv0005ad9lbbo7d19c","tag_id":"cmf9mvzsx000cad9laim14rvs","_id":"cmf9mvzsx000jad9la4818bc4"},{"post_id":"cmf9mvzsv0005ad9lbbo7d19c","tag_id":"cmf9mvzsx000gad9l8usj7r62","_id":"cmf9mvzsx000kad9lhm7q40ts"},{"post_id":"cmf9mvzsv0006ad9l65l0eowv","tag_id":"cmf9mvzsx000iad9l0z7ehx71","_id":"cmf9mvzsy000lad9ld7z7gpgi"}],"Tag":[{"name":"留学申请","_id":"cmf9mvzsv0004ad9lg3x17vpo"},{"name":"转载","_id":"cmf9mvzsx000cad9laim14rvs"},{"name":"pytorch","_id":"cmf9mvzsx000gad9l8usj7r62"},{"name":"摄影","_id":"cmf9mvzsx000iad9l0z7ehx71"}]}}